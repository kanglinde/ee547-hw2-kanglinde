[
  {
    "arxiv_id": "9905014v1",
    "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function\n  Decomposition",
    "authors": [
      "Thomas G. Dietterich"
    ],
    "abstract": "  This paper presents the MAXQ approach to hierarchical reinforcement learning\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\nof smaller MDPs and decomposing the value function of the target MDP into an\nadditive combination of the value functions of the smaller MDPs. The paper\ndefines the MAXQ hierarchy, proves formal results on its representational\npower, and establishes five conditions for the safe use of state abstractions.\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\nthat it converges wih probability 1 to a kind of locally-optimal policy known\nas a recursively optimal policy, even in the presence of the five kinds of\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\nthrough a series of experiments in three domains and shows experimentally that\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\nvalue function has an important benefit: it makes it possible to compute and\nexecute an improved, non-hierarchical policy via a procedure similar to the\npolicy improvement step of policy iteration. The paper demonstrates the\neffectiveness of this non-hierarchical execution experimentally. Finally, the\npaper concludes with a comparison to related work and a discussion of the\ndesign tradeoffs in hierarchical reinforcement learning.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "1999-05-21T14:26:07Z",
    "updated": "1999-05-21T14:26:07Z",
    "abstract_stats": {
      "total_words": 216,
      "unique_words": 114,
      "total_sentences": 7,
      "avg_words_per_sentence": 30.857142857142858,
      "avg_word_length": 5.597222222222222
    }
  },
  {
    "arxiv_id": "9905015v1",
    "title": "State Abstraction in MAXQ Hierarchical Reinforcement Learning",
    "authors": [
      "Thomas G. Dietterich"
    ],
    "abstract": "  Many researchers have explored methods for hierarchical reinforcement\nlearning (RL) with temporal abstractions, in which abstract actions are defined\nthat can perform many primitive actions before terminating. However, little is\nknown about learning with state abstractions, in which aspects of the state\nspace are ignored. In previous work, we developed the MAXQ method for\nhierarchical RL. In this paper, we define five conditions under which state\nabstraction can be combined with the MAXQ value function decomposition. We\nprove that the MAXQ-Q learning algorithm converges under these conditions and\nshow experimentally that state abstraction is important for the successful\napplication of MAXQ-Q learning.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "1999-05-21T14:49:39Z",
    "updated": "1999-05-21T14:49:39Z",
    "abstract_stats": {
      "total_words": 102,
      "unique_words": 66,
      "total_sentences": 5,
      "avg_words_per_sentence": 20.4,
      "avg_word_length": 5.872549019607843
    }
  },
  {
    "arxiv_id": "0001004v1",
    "title": "Multiplicative Algorithm for Orthgonal Groups and Independent Component\n  Analysis",
    "authors": [
      "Toshinao Akuzawa"
    ],
    "abstract": "  The multiplicative Newton-like method developed by the author et al. is\nextended to the situation where the dynamics is restricted to the orthogonal\ngroup. A general framework is constructed without specifying the cost function.\nThough the restriction to the orthogonal groups makes the problem somewhat\ncomplicated, an explicit expression for the amount of individual jumps is\nobtained. This algorithm is exactly second-order-convergent. The global\ninstability inherent in the Newton method is remedied by a\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\napplied to the independent component analysis. Its remarkable performance is\nillustrated by a numerical simulation.\n",
    "categories": [
      "cs.LG",
      "G.1.6"
    ],
    "published": "2000-01-07T06:20:53Z",
    "updated": "2000-01-07T06:20:53Z",
    "abstract_stats": {
      "total_words": 98,
      "unique_words": 68,
      "total_sentences": 8,
      "avg_words_per_sentence": 12.25,
      "avg_word_length": 6.091836734693878
    }
  },
  {
    "arxiv_id": "0002006v1",
    "title": "Multiplicative Nonholonomic/Newton -like Algorithm",
    "authors": [
      "Toshinao Akuzawa",
      "Noboru Murata"
    ],
    "abstract": "  We construct new algorithms from scratch, which use the fourth order cumulant\nof stochastic variables for the cost function. The multiplicative updating rule\nhere constructed is natural from the homogeneous nature of the Lie group and\nhas numerous merits for the rigorous treatment of the dynamics. As one\nconsequence, the second order convergence is shown. For the cost function,\nfunctions invariant under the componentwise scaling are choosen. By identifying\npoints which can be transformed to each other by the scaling, we assume that\nthe dynamics is in a coset space. In our method, a point can move toward any\ndirection in this coset. Thus, no prewhitening is required.\n",
    "categories": [
      "cs.LG",
      "G.1.6"
    ],
    "published": "2000-02-09T06:44:28Z",
    "updated": "2000-02-09T06:44:28Z",
    "abstract_stats": {
      "total_words": 108,
      "unique_words": 76,
      "total_sentences": 7,
      "avg_words_per_sentence": 15.428571428571429,
      "avg_word_length": 5.12962962962963
    }
  },
  {
    "arxiv_id": "0009001v3",
    "title": "Complexity analysis for algorithmically simple strings",
    "authors": [
      "Andrei N. Soklakov"
    ],
    "abstract": "  Given a reference computer, Kolmogorov complexity is a well defined function\non all binary strings. In the standard approach, however, only the asymptotic\nproperties of such functions are considered because they do not depend on the\nreference computer. We argue that this approach can be more useful if it is\nrefined to include an important practical case of simple binary strings.\nKolmogorov complexity calculus may be developed for this case if we restrict\nthe class of available reference computers. The interesting problem is to\ndefine a class of computers which is restricted in a {\\it natural} way modeling\nthe real-life situation where only a limited class of computers is physically\navailable to us. We give an example of what such a natural restriction might\nlook like mathematically, and show that under such restrictions some error\nterms, even logarithmic in complexity, can disappear from the standard\ncomplexity calculus.\n  Keywords: Kolmogorov complexity; Algorithmic information theory.\n",
    "categories": [
      "cs.LG",
      "E.4; F.2; I.2"
    ],
    "published": "2000-09-05T18:54:58Z",
    "updated": "2002-02-26T01:51:09Z",
    "abstract_stats": {
      "total_words": 153,
      "unique_words": 97,
      "total_sentences": 7,
      "avg_words_per_sentence": 21.857142857142858,
      "avg_word_length": 5.444444444444445
    }
  },
  {
    "arxiv_id": "0009007v1",
    "title": "Robust Classification for Imprecise Environments",
    "authors": [
      "Foster Provost",
      "Tom Fawcett"
    ],
    "abstract": "  In real-world environments it usually is difficult to specify target\noperating conditions precisely, for example, target misclassification costs.\nThis uncertainty makes building robust classification systems problematic. We\nshow that it is possible to build a hybrid classifier that will perform at\nleast as well as the best available classifier for any target conditions. In\nsome cases, the performance of the hybrid actually can surpass that of the best\nknown classifier. This robust performance extends across a wide variety of\ncomparison frameworks, including the optimization of metrics such as accuracy,\nexpected cost, lift, precision, recall, and workforce utilization. The hybrid\nalso is efficient to build, to store, and to update. The hybrid is based on a\nmethod for the comparison of classifier performance that is robust to imprecise\nclass distributions and misclassification costs. The ROC convex hull (ROCCH)\nmethod combines techniques from ROC analysis, decision analysis and\ncomputational geometry, and adapts them to the particulars of analyzing learned\nclassifiers. The method is efficient and incremental, minimizes the management\nof classifier performance data, and allows for clear visual comparisons and\nsensitivity analyses. Finally, we point to empirical evidence that a robust\nhybrid classifier indeed is needed for many real-world problems.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2000-09-13T21:09:47Z",
    "updated": "2000-09-13T21:09:47Z",
    "abstract_stats": {
      "total_words": 198,
      "unique_words": 115,
      "total_sentences": 10,
      "avg_words_per_sentence": 19.8,
      "avg_word_length": 5.7727272727272725
    }
  },
  {
    "arxiv_id": "0011032v1",
    "title": "Top-down induction of clustering trees",
    "authors": [
      "Hendrik Blockeel",
      "Luc De Raedt",
      "Jan Ramon"
    ],
    "abstract": "  An approach to clustering is presented that adapts the basic top-down\ninduction of decision trees method towards clustering. To this aim, it employs\nthe principles of instance based learning. The resulting methodology is\nimplemented in the TIC (Top down Induction of Clustering trees) system for\nfirst order clustering. The TIC system employs the first order logical decision\ntree representation of the inductive logic programming system Tilde. Various\nexperiments with TIC are presented, in both propositional and relational\ndomains.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2000-11-21T21:51:01Z",
    "updated": "2000-11-21T21:51:01Z",
    "abstract_stats": {
      "total_words": 78,
      "unique_words": 53,
      "total_sentences": 5,
      "avg_words_per_sentence": 15.6,
      "avg_word_length": 5.769230769230769
    }
  },
  {
    "arxiv_id": "0011044v1",
    "title": "Scaling Up Inductive Logic Programming by Learning from Interpretations",
    "authors": [
      "Hendrik Blockeel",
      "Luc De Raedt",
      "Nico Jacobs",
      "Bart Demoen"
    ],
    "abstract": "  When comparing inductive logic programming (ILP) and attribute-value learning\ntechniques, there is a trade-off between expressive power and efficiency.\nInductive logic programming techniques are typically more expressive but also\nless efficient. Therefore, the data sets handled by current inductive logic\nprogramming systems are small according to general standards within the data\nmining community. The main source of inefficiency lies in the assumption that\nseveral examples may be related to each other, so they cannot be handled\nindependently.\n  Within the learning from interpretations framework for inductive logic\nprogramming this assumption is unnecessary, which allows to scale up existing\nILP algorithms. In this paper we explain this learning setting in the context\nof relational databases. We relate the setting to propositional data mining and\nto the classical ILP setting, and show that learning from interpretations\ncorresponds to learning from multiple relations and thus extends the\nexpressiveness of propositional learning, while maintaining its efficiency to a\nlarge extent (which is not the case in the classical ILP setting).\n  As a case study, we present two alternative implementations of the ILP system\nTilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which\nloads all data in main memory, and Tilde-LDS, which loads the examples one by\none. We experimentally compare the implementations, showing Tilde-LDS can\nhandle large data sets (in the order of 100,000 examples or 100 MB) and indeed\nscales up linearly in the number of examples.\n",
    "categories": [
      "cs.LG",
      "I.2.6 ; I.2.3"
    ],
    "published": "2000-11-29T12:14:50Z",
    "updated": "2000-11-29T12:14:50Z",
    "abstract_stats": {
      "total_words": 234,
      "unique_words": 133,
      "total_sentences": 9,
      "avg_words_per_sentence": 26.0,
      "avg_word_length": 5.64957264957265
    }
  },
  {
    "arxiv_id": "0103003v1",
    "title": "Learning Policies with External Memory",
    "authors": [
      "Leonid Peshkin",
      "Nicolas Meuleau",
      "Leslie Kaelbling"
    ],
    "abstract": "  In order for an agent to perform well in partially observable domains, it is\nusually necessary for actions to depend on the history of observations. In this\npaper, we explore a {\\it stigmergic} approach, in which the agent's actions\ninclude the ability to set and clear bits in an external memory, and the\nexternal memory is included as part of the input to the agent. In this case, we\nneed to learn a reactive policy in a highly non-Markovian domain. We explore\ntwo algorithms: SARSA(\\lambda), which has had empirical success in partially\nobservable domains, and VAPS, a new algorithm due to Baird and Moore, with\nconvergence guarantees in partially observable domains. We compare the\nperformance of these two algorithms on benchmark problems.\n",
    "categories": [
      "cs.LG",
      "I.2.8;I.2.6;I.2.11;I.2;I.2.3"
    ],
    "published": "2001-03-02T01:55:46Z",
    "updated": "2001-03-02T01:55:46Z",
    "abstract_stats": {
      "total_words": 122,
      "unique_words": 74,
      "total_sentences": 5,
      "avg_words_per_sentence": 24.4,
      "avg_word_length": 4.991803278688525
    }
  },
  {
    "arxiv_id": "0110036v1",
    "title": "Efficient algorithms for decision tree cross-validation",
    "authors": [
      "Hendrik Blockeel",
      "Jan Struyf"
    ],
    "abstract": "  Cross-validation is a useful and generally applicable technique often\nemployed in machine learning, including decision tree induction. An important\ndisadvantage of straightforward implementation of the technique is its\ncomputational overhead. In this paper we show that, for decision trees, the\ncomputational overhead of cross-validation can be reduced significantly by\nintegrating the cross-validation with the normal decision tree induction\nprocess. We discuss how existing decision tree algorithms can be adapted to\nthis aim, and provide an analysis of the speedups these adaptations may yield.\nThe analysis is supported by experimental results.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2001-10-17T15:45:23Z",
    "updated": "2001-10-17T15:45:23Z",
    "abstract_stats": {
      "total_words": 90,
      "unique_words": 60,
      "total_sentences": 5,
      "avg_words_per_sentence": 18.0,
      "avg_word_length": 6.111111111111111
    }
  },
  {
    "arxiv_id": "0211003v1",
    "title": "Evaluation of the Performance of the Markov Blanket Bayesian Classifier\n  Algorithm",
    "authors": [
      "Michael G. Madden"
    ],
    "abstract": "  The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for\nconstruction of probabilistic classifiers. This paper presents an empirical\ncomparison of the MBBC algorithm with three other Bayesian classifiers: Naive\nBayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these\nare implemented using the K2 framework of Cooper and Herskovits. The\nclassifiers are compared in terms of their performance (using simple accuracy\nmeasures and ROC curves) and speed, on a range of standard benchmark data sets.\nIt is concluded that MBBC is competitive in terms of speed and accuracy with\nthe other algorithms considered.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2002-11-01T18:09:56Z",
    "updated": "2002-11-01T18:09:56Z",
    "abstract_stats": {
      "total_words": 96,
      "unique_words": 64,
      "total_sentences": 5,
      "avg_words_per_sentence": 19.2,
      "avg_word_length": 5.65625
    }
  },
  {
    "arxiv_id": "0211007v1",
    "title": "Approximating Incomplete Kernel Matrices by the em Algorithm",
    "authors": [
      "Koji Tsuda",
      "Shotaro Akaho",
      "Kiyoshi Asai"
    ],
    "abstract": "  In biological data, it is often the case that observed data are available\nonly for a subset of samples. When a kernel matrix is derived from such data,\nwe have to leave the entries for unavailable samples as missing. In this paper,\nwe make use of a parametric model of kernel matrices, and estimate missing\nentries by fitting the model to existing entries. The parametric model is\ncreated as a set of spectral variants of a complete kernel matrix derived from\nanother information source. For model fitting, we adopt the em algorithm based\non the information geometry of positive definite matrices. We will report\npromising results on bacteria clustering experiments using two marker\nsequences: 16S and gyrB.\n",
    "categories": [
      "cs.LG",
      "I2.6; I5.2"
    ],
    "published": "2002-11-07T07:21:58Z",
    "updated": "2002-11-07T07:21:58Z",
    "abstract_stats": {
      "total_words": 117,
      "unique_words": 73,
      "total_sentences": 6,
      "avg_words_per_sentence": 19.5,
      "avg_word_length": 4.957264957264957
    }
  },
  {
    "arxiv_id": "0309015v1",
    "title": "Reliable and Efficient Inference of Bayesian Networks from Sparse Data\n  by Statistical Learning Theory",
    "authors": [
      "Dominik Janzing",
      "Daniel Herrmann"
    ],
    "abstract": "  To learn (statistical) dependencies among random variables requires\nexponentially large sample size in the number of observed random variables if\nany arbitrary joint probability distribution can occur.\n  We consider the case that sparse data strongly suggest that the probabilities\ncan be described by a simple Bayesian network, i.e., by a graph with small\nin-degree \\Delta. Then this simple law will also explain further data with high\nconfidence. This is shown by calculating bounds on the VC dimension of the set\nof those probability measures that correspond to simple graphs. This allows to\nselect networks by structural risk minimization and gives reliability bounds on\nthe error of the estimated joint measure without (in contrast to a previous\npaper) any prior assumptions on the set of possible joint measures.\n  The complexity for searching the optimal Bayesian networks of in-degree\n\\Delta increases only polynomially in the number of random varibales for\nconstant \\Delta and the optimal joint measure associated with a given graph can\nbe found by convex optimization.\n",
    "categories": [
      "cs.LG",
      "K.3.2"
    ],
    "published": "2003-09-10T13:56:41Z",
    "updated": "2003-09-10T13:56:41Z",
    "abstract_stats": {
      "total_words": 167,
      "unique_words": 103,
      "total_sentences": 8,
      "avg_words_per_sentence": 20.875,
      "avg_word_length": 5.383233532934132
    }
  },
  {
    "arxiv_id": "0311042v1",
    "title": "Toward Attribute Efficient Learning Algorithms",
    "authors": [
      "Adam R. Klivans",
      "Rocco A. Servedio"
    ],
    "abstract": "  We make progress on two important problems regarding attribute efficient\nlearnability.\n  First, we give an algorithm for learning decision lists of length $k$ over\n$n$ variables using $2^{\\tilde{O}(k^{1/3})} \\log n$ examples and time\n$n^{\\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision\nlists that has both subexponential sample complexity and subexponential running\ntime in the relevant parameters. Our approach establishes a relationship\nbetween attribute efficient learning and polynomial threshold functions and is\nbased on a new construction of low degree, low weight polynomial threshold\nfunctions for decision lists. For a wide range of parameters our construction\nmatches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives\nan essentially optimal tradeoff between polynomial threshold function degree\nand weight.\n  Second, we give an algorithm for learning an unknown parity function on $k$\nout of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.\nFor $k=o(\\log n)$ this yields a polynomial time algorithm with sample\ncomplexity $o(n)$. This is the first polynomial time algorithm for learning\nparity on a superconstant number of variables with sublinear sample complexity.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2003-11-27T05:34:04Z",
    "updated": "2003-11-27T05:34:04Z",
    "abstract_stats": {
      "total_words": 179,
      "unique_words": 90,
      "total_sentences": 8,
      "avg_words_per_sentence": 22.375,
      "avg_word_length": 5.871508379888268
    }
  },
  {
    "arxiv_id": "0312004v1",
    "title": "Improving spam filtering by combining Naive Bayes with simple k-nearest\n  neighbor searches",
    "authors": [
      "Daniel Etzold"
    ],
    "abstract": "  Using naive Bayes for email classification has become very popular within the\nlast few months. They are quite easy to implement and very efficient. In this\npaper we want to present empirical results of email classification using a\ncombination of naive Bayes and k-nearest neighbor searches. Using this\ntechnique we show that the accuracy of a Bayes filter can be improved slightly\nfor a high number of features and significantly for a small number of features.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2003-11-30T20:41:18Z",
    "updated": "2003-11-30T20:41:18Z",
    "abstract_stats": {
      "total_words": 76,
      "unique_words": 51,
      "total_sentences": 4,
      "avg_words_per_sentence": 19.0,
      "avg_word_length": 5.0131578947368425
    }
  },
  {
    "arxiv_id": "0401005v1",
    "title": "About Unitary Rating Score Constructing",
    "authors": [
      "Kromer Victor"
    ],
    "abstract": "  It is offered to pool test points of different subjects and different aspects\nof the same subject together in order to get the unitary rating score, by the\nway of nonlinear transformation of indicator points in accordance with Zipf's\ndistribution. It is proposed to use the well-studied distribution of\nIntellectuality Quotient IQ as the reference distribution for latent variable\n\"progress in studies\".\n",
    "categories": [
      "cs.LG",
      "1.2.6"
    ],
    "published": "2004-01-08T07:50:51Z",
    "updated": "2004-01-08T07:50:51Z",
    "abstract_stats": {
      "total_words": 62,
      "unique_words": 44,
      "total_sentences": 2,
      "avg_words_per_sentence": 31.0,
      "avg_word_length": 5.467741935483871
    }
  },
  {
    "arxiv_id": "0412003v1",
    "title": "Mining Heterogeneous Multivariate Time-Series for Learning Meaningful\n  Patterns: Application to Home Health Telecare",
    "authors": [
      "Florence Duchene",
      "Catherine Garbay",
      "Vincent Rialle"
    ],
    "abstract": "  For the last years, time-series mining has become a challenging issue for\nresearchers. An important application lies in most monitoring purposes, which\nrequire analyzing large sets of time-series for learning usual patterns. Any\ndeviation from this learned profile is then considered as an unexpected\nsituation. Moreover, complex applications may involve the temporal study of\nseveral heterogeneous parameters. In that paper, we propose a method for mining\nheterogeneous multivariate time-series for learning meaningful patterns. The\nproposed approach allows for mixed time-series -- containing both pattern and\nnon-pattern data -- such as for imprecise matches, outliers, stretching and\nglobal translating of patterns instances in time. We present the early results\nof our approach in the context of monitoring the health status of a person at\nhome. The purpose is to build a behavioral profile of a person by analyzing the\ntime variations of several quantitative or qualitative parameters recorded\nthrough a provision of sensors installed in the home.\n",
    "categories": [
      "cs.LG",
      "G.3"
    ],
    "published": "2004-12-01T16:32:49Z",
    "updated": "2004-12-01T16:32:49Z",
    "abstract_stats": {
      "total_words": 156,
      "unique_words": 102,
      "total_sentences": 8,
      "avg_words_per_sentence": 19.5,
      "avg_word_length": 5.666666666666667
    }
  },
  {
    "arxiv_id": "0502016v1",
    "title": "Stability Analysis for Regularized Least Squares Regression",
    "authors": [
      "Cynthia Rudin"
    ],
    "abstract": "  We discuss stability for a class of learning algorithms with respect to noisy\nlabels. The algorithms we consider are for regression, and they involve the\nminimization of regularized risk functionals, such as L(f) := 1/N sum_i\n(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\ny_i is a noisy version of f*(x_i) for some function f* in H, the output of the\nalgorithm converges to f* as the regularization term and noise simultaneously\nvanish. We consider two flavors of this problem, one where a data set of N\npoints remains fixed, and the other where N -> infinity. For the case where N\n-> infinity, we give conditions for convergence to f_E (the function which is\nthe expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\ndescribe the limiting 'non-noisy', 'non-regularized' function f*, and give\nconditions for convergence. In the process, we develop a set of tools for\ndealing with functionals such as L(f), which are applicable to many other\nproblems in learning theory.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-02-03T19:54:02Z",
    "updated": "2005-02-03T19:54:02Z",
    "abstract_stats": {
      "total_words": 173,
      "unique_words": 95,
      "total_sentences": 7,
      "avg_words_per_sentence": 24.714285714285715,
      "avg_word_length": 4.803468208092486
    }
  },
  {
    "arxiv_id": "0504001v1",
    "title": "Probabilistic and Team PFIN-type Learning: General Properties",
    "authors": [
      "Andris Ambainis"
    ],
    "abstract": "  We consider the probability hierarchy for Popperian FINite learning and study\nthe general properties of this hierarchy. We prove that the probability\nhierarchy is decidable, i.e. there exists an algorithm that receives p_1 and\np_2 and answers whether PFIN-type learning with the probability of success p_1\nis equivalent to PFIN-type learning with the probability of success p_2.\n  To prove our result, we analyze the topological structure of the probability\nhierarchy. We prove that it is well-ordered in descending ordering and\norder-equivalent to ordinal epsilon_0. This shows that the structure of the\nhierarchy is very complicated.\n  Using similar methods, we also prove that, for PFIN-type learning, team\nlearning and probabilistic learning are of the same power.\n",
    "categories": [
      "cs.LG",
      "F.1.1, I.2.6"
    ],
    "published": "2005-03-31T23:04:28Z",
    "updated": "2005-03-31T23:04:28Z",
    "abstract_stats": {
      "total_words": 116,
      "unique_words": 60,
      "total_sentences": 8,
      "avg_words_per_sentence": 14.5,
      "avg_word_length": 5.491379310344827
    }
  },
  {
    "arxiv_id": "0506004v4",
    "title": "Non-asymptotic calibration and resolution",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  We analyze a new algorithm for probability forecasting of binary observations\non the basis of the available data, without making any assumptions about the\nway the observations are generated. The algorithm is shown to be well\ncalibrated and to have good resolution for long enough sequences of\nobservations and for a suitable choice of its parameter, a kernel on the\nCartesian product of the forecast space $[0,1]$ and the data space. Our main\nresults are non-asymptotic: we establish explicit inequalities, shown to be\ntight, for the performance of the algorithm.\n",
    "categories": [
      "cs.LG",
      "I.2.6; I.5.1"
    ],
    "published": "2005-06-01T14:03:20Z",
    "updated": "2006-07-01T13:46:30Z",
    "abstract_stats": {
      "total_words": 91,
      "unique_words": 57,
      "total_sentences": 3,
      "avg_words_per_sentence": 30.333333333333332,
      "avg_word_length": 5.1208791208791204
    }
  },
  {
    "arxiv_id": "0506007v2",
    "title": "Defensive forecasting for linear protocols",
    "authors": [
      "Vladimir Vovk",
      "Ilia Nouretdinov",
      "Akimichi Takemura",
      "Glenn Shafer"
    ],
    "abstract": "  We consider a general class of forecasting protocols, called \"linear\nprotocols\", and discuss several important special cases, including multi-class\nforecasting. Forecasting is formalized as a game between three players:\nReality, whose role is to generate observations; Forecaster, whose goal is to\npredict the observations; and Skeptic, who tries to make money on any lack of\nagreement between Forecaster's predictions and the actual observations. Our\nmain mathematical result is that for any continuous strategy for Skeptic in a\nlinear protocol there exists a strategy for Forecaster that does not allow\nSkeptic's capital to grow. This result is a meta-theorem that allows one to\ntransform any continuous law of probability in a linear protocol into a\nforecasting strategy whose predictions are guaranteed to satisfy this law. We\napply this meta-theorem to a weak law of large numbers in Hilbert spaces to\nobtain a version of the K29 prediction algorithm for linear protocols and show\nthat this version also satisfies the attractive properties of proper\ncalibration and resolution under a suitable choice of its kernel parameter,\nwith no assumptions about the way the data is generated.\n",
    "categories": [
      "cs.LG",
      "I.2.6; I.5.1"
    ],
    "published": "2005-06-02T13:26:43Z",
    "updated": "2005-09-24T16:55:14Z",
    "abstract_stats": {
      "total_words": 183,
      "unique_words": 110,
      "total_sentences": 5,
      "avg_words_per_sentence": 36.6,
      "avg_word_length": 5.415300546448087
    }
  },
  {
    "arxiv_id": "0506057v2",
    "title": "About one 3-parameter Model of Testing",
    "authors": [
      "Kromer Victor"
    ],
    "abstract": "  This article offers a 3-parameter model of testing, with 1) the difference\nbetween the ability level of the examinee and item difficulty; 2) the examinee\ndiscrimination and 3) the item discrimination as model parameters.\n",
    "categories": [
      "cs.LG",
      "I.2.6; K.3.2"
    ],
    "published": "2005-06-14T04:00:38Z",
    "updated": "2005-07-21T02:43:12Z",
    "abstract_stats": {
      "total_words": 34,
      "unique_words": 24,
      "total_sentences": 1,
      "avg_words_per_sentence": 34.0,
      "avg_word_length": 5.411764705882353
    }
  },
  {
    "arxiv_id": "0506085v1",
    "title": "On the Job Training",
    "authors": [
      "Jason E. Holt"
    ],
    "abstract": "  We propose a new framework for building and evaluating machine learning\nalgorithms. We argue that many real-world problems require an agent which must\nquickly learn to respond to demands, yet can continue to perform and respond to\nnew training throughout its useful life. We give a framework for how such\nagents can be built, describe several metrics for evaluating them, and show\nthat subtle changes in system construction can significantly affect agent\nperformance.\n",
    "categories": [
      "cs.LG",
      "K.3.2"
    ],
    "published": "2005-06-22T21:21:13Z",
    "updated": "2005-06-22T21:21:13Z",
    "abstract_stats": {
      "total_words": 73,
      "unique_words": 55,
      "total_sentences": 3,
      "avg_words_per_sentence": 24.333333333333332,
      "avg_word_length": 5.328767123287672
    }
  },
  {
    "arxiv_id": "0507033v2",
    "title": "Multiresolution Kernels",
    "authors": [
      "Marco Cuturi",
      "Kenji Fukumizu"
    ],
    "abstract": "  We present in this work a new methodology to design kernels on data which is\nstructured with smaller components, such as text, images or sequences. This\nmethodology is a template procedure which can be applied on most kernels on\nmeasures and takes advantage of a more detailed \"bag of components\"\nrepresentation of the objects. To obtain such a detailed description, we\nconsider possible decompositions of the original bag into a collection of\nnested bags, following a prior knowledge on the objects' structure. We then\nconsider these smaller bags to compare two objects both in a detailed\nperspective, stressing local matches between the smaller bags, and in a global\nor coarse perspective, by considering the entire bag. This multiresolution\napproach is likely to be best suited for tasks where the coarse approach is not\nprecise enough, and where a more subtle mixture of both local and global\nsimilarities is necessary to compare objects. The approach presented here would\nnot be computationally tractable without a factorization trick that we\nintroduce before presenting promising results on an image retrieval task.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-07-13T05:45:28Z",
    "updated": "2005-11-14T08:18:49Z",
    "abstract_stats": {
      "total_words": 177,
      "unique_words": 107,
      "total_sentences": 6,
      "avg_words_per_sentence": 29.5,
      "avg_word_length": 5.259887005649717
    }
  },
  {
    "arxiv_id": "0507044v1",
    "title": "Defensive Universal Learning with Experts",
    "authors": [
      "Jan Poland",
      "Marcus Hutter"
    ],
    "abstract": "  This paper shows how universal learning can be achieved with expert advice.\nTo this aim, we specify an experts algorithm with the following\ncharacteristics: (a) it uses only feedback from the actions actually chosen\n(bandit setup), (b) it can be applied with countably infinite expert classes,\nand (c) it copes with losses that may grow in time appropriately slowly. We\nprove loss bounds against an adaptive adversary. From this, we obtain a master\nalgorithm for \"reactive\" experts problems, which means that the master's\nactions may influence the behavior of the adversary. Our algorithm can\nsignificantly outperform standard experts algorithms on such problems. Finally,\nwe combine it with a universal expert class. The resulting universal learner\nperforms -- in a certain sense -- almost as well as any computable strategy,\nfor any online decision problem. We also specify the (worst-case) convergence\nspeed, which is very slow.\n",
    "categories": [
      "cs.LG",
      "I.2.6; G.3"
    ],
    "published": "2005-07-18T14:33:56Z",
    "updated": "2005-07-18T14:33:56Z",
    "abstract_stats": {
      "total_words": 145,
      "unique_words": 99,
      "total_sentences": 8,
      "avg_words_per_sentence": 18.125,
      "avg_word_length": 5.317241379310345
    }
  },
  {
    "arxiv_id": "0507062v1",
    "title": "FPL Analysis for Adaptive Bandits",
    "authors": [
      "Jan Poland"
    ],
    "abstract": "  A main problem of \"Follow the Perturbed Leader\" strategies for online\ndecision problems is that regret bounds are typically proven against oblivious\nadversary. In partial observation cases, it was not clear how to obtain\nperformance guarantees against adaptive adversary, without worsening the\nbounds. We propose a conceptually simple argument to resolve this problem.\nUsing this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed\nbandit problem is shown. This bound holds for the common FPL variant using only\nthe observations from designated exploration rounds. Using all observations\nallows for the stronger bound of O(t^(1/2)), matching the best bound known so\nfar (and essentially the known lower bound) for adversarial bandits.\nSurprisingly, this variant does not even need explicit exploration, it is\nself-stabilizing. However the sampling probabilities have to be either\nexternally provided or approximated to sufficient accuracy, using O(t^2 log t)\nsamples in each step.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-07-26T05:00:27Z",
    "updated": "2005-07-26T05:00:27Z",
    "abstract_stats": {
      "total_words": 147,
      "unique_words": 101,
      "total_sentences": 8,
      "avg_words_per_sentence": 18.375,
      "avg_word_length": 5.687074829931973
    }
  },
  {
    "arxiv_id": "0509055v1",
    "title": "Learning Optimal Augmented Bayes Networks",
    "authors": [
      "Vikas Hamine",
      "Paul Helman"
    ],
    "abstract": "  Naive Bayes is a simple Bayesian classifier with strong independence\nassumptions among the attributes. This classifier, desipte its strong\nindependence assumptions, often performs well in practice. It is believed that\nrelaxing the independence assumptions of a naive Bayes classifier may improve\nthe classification accuracy of the resulting structure. While finding an\noptimal unconstrained Bayesian Network (for most any reasonable scoring\nmeasure) is an NP-hard problem, it is possible to learn in polynomial time\noptimal networks obeying various structural restrictions. Several authors have\nexamined the possibilities of adding augmenting arcs between attributes of a\nNaive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN\nstructure in which the augmenting arcs form a tree on the attributes, and\npresent a polynomial time algorithm that learns an optimal TAN with respect to\nMDL score. Keogh and Pazzani define Augmented Bayes Networks in which the\naugmenting arcs form a forest on the attributes (a collection of trees, hence a\nrelaxation of the stuctural restriction of TAN), and present heuristic search\nmethods for learning good, though not optimal, augmenting arc sets. The\nauthors, however, evaluate the learned structure only in terms of observed\nmisclassification error and not against a scoring metric, such as MDL. In this\npaper, we present a simple, polynomial time greedy algorithm for learning an\noptimal Augmented Bayes Network with respect to MDL score.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-09-19T04:57:26Z",
    "updated": "2005-09-19T04:57:26Z",
    "abstract_stats": {
      "total_words": 222,
      "unique_words": 119,
      "total_sentences": 9,
      "avg_words_per_sentence": 24.666666666666668,
      "avg_word_length": 5.608108108108108
    }
  },
  {
    "arxiv_id": "0510038v4",
    "title": "Learning Unions of $\u03c9(1)$-Dimensional Rectangles",
    "authors": [
      "Alp Atici",
      "Rocco A. Servedio"
    ],
    "abstract": "  We consider the problem of learning unions of rectangles over the domain\n$[b]^n$, in the uniform distribution membership query learning setting, where\nboth b and n are \"large\". We obtain poly$(n, \\log b)$-time algorithms for the\nfollowing classes:\n  - poly$(n \\log b)$-way Majority of $O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log\nb)})$-dimensional rectangles.\n  - Union of poly$(\\log(n \\log b))$ many $O(\\frac{\\log^2 (n \\log b)} {(\\log\n\\log(n \\log b) \\log \\log \\log (n \\log b))^2})$-dimensional rectangles.\n  - poly$(n \\log b)$-way Majority of poly$(n \\log b)$-Or of disjoint\n$O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log b)})$-dimensional rectangles.\n  Our main algorithmic tool is an extension of Jackson's boosting- and\nFourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$,\nbuilding on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to\nobtain the results stated above are techniques from exact learning [Beimel,\nKushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$\ncircuits [Jackson, Klivans, Servedio 2002] and on representing Boolean\nfunctions as thresholds of parities [Klivans, Servedio 2001].\n",
    "categories": [
      "cs.LG",
      "F.2.2; I.2.6"
    ],
    "published": "2005-10-14T19:26:34Z",
    "updated": "2007-06-26T14:00:17Z",
    "abstract_stats": {
      "total_words": 165,
      "unique_words": 102,
      "total_sentences": 6,
      "avg_words_per_sentence": 27.5,
      "avg_word_length": 5.884848484848485
    }
  },
  {
    "arxiv_id": "0511058v2",
    "title": "On-line regression competitive with reproducing kernel Hilbert spaces",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  We consider the problem of on-line prediction of real-valued labels, assumed\nbounded in absolute value by a known constant, of new objects from known\nlabeled objects. The prediction algorithm's performance is measured by the\nsquared deviation of the predictions from the actual labels. No stochastic\nassumptions are made about the way the labels and objects are generated.\nInstead, we are given a benchmark class of prediction rules some of which are\nhoped to produce good predictions. We show that for a wide range of\ninfinite-dimensional benchmark classes one can construct a prediction algorithm\nwhose cumulative loss over the first N examples does not exceed the cumulative\nloss of any prediction rule in the class plus O(sqrt(N)); the main differences\nfrom the known results are that we do not impose any upper bound on the norm of\nthe considered prediction rules and that we achieve an optimal leading term in\nthe excess loss of our algorithm. If the benchmark class is \"universal\" (dense\nin the class of continuous functions on each compact set), this provides an\non-line non-stochastic analogue of universally consistent prediction in\nnon-parametric statistics. We use two proof techniques: one is based on the\nAggregating Algorithm and the other on the recently developed method of\ndefensive forecasting.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-11-15T17:13:50Z",
    "updated": "2006-01-24T23:27:14Z",
    "abstract_stats": {
      "total_words": 208,
      "unique_words": 120,
      "total_sentences": 7,
      "avg_words_per_sentence": 29.714285714285715,
      "avg_word_length": 5.259615384615385
    }
  },
  {
    "arxiv_id": "0511088v1",
    "title": "Bounds on Query Convergence",
    "authors": [
      "Barak A. Pearlmutter"
    ],
    "abstract": "  The problem of finding an optimum using noisy evaluations of a smooth cost\nfunction arises in many contexts, including economics, business, medicine,\nexperiment design, and foraging theory. We derive an asymptotic bound E[ (x_t -\nx*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1,\n>...) generated by an unbiased feedback process observing noisy evaluations of\nan unknown quadratic function maximised at x*. The bound is tight, as the proof\nleads to a simple algorithm which meets it. We further establish a bound on the\ntotal regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may\nimpose practical limitations on an agent's performance, as O(eps^-4) queries\nare made before the queries converge to x* with eps accuracy.\n",
    "categories": [
      "cs.LG",
      "G.1.6"
    ],
    "published": "2005-11-25T15:57:56Z",
    "updated": "2005-11-25T15:57:56Z",
    "abstract_stats": {
      "total_words": 125,
      "unique_words": 92,
      "total_sentences": 6,
      "avg_words_per_sentence": 20.833333333333332,
      "avg_word_length": 4.944
    }
  },
  {
    "arxiv_id": "0512050v1",
    "title": "Preference Learning in Terminology Extraction: A ROC-based approach",
    "authors": [
      "J\u00e9r\u00f4me Az\u00e9",
      "Mathieu Roche",
      "Yves Kodratoff",
      "Mich\u00e8le Sebag"
    ],
    "abstract": "  A key data preparation step in Text Mining, Term Extraction selects the\nterms, or collocation of words, attached to specific concepts. In this paper,\nthe task of extracting relevant collocations is achieved through a supervised\nlearning algorithm, exploiting a few collocations manually labelled as\nrelevant/irrelevant. The candidate terms are described along 13 standard\nstatistical criteria measures. From these examples, an evolutionary learning\nalgorithm termed Roger, based on the optimization of the Area under the ROC\ncurve criterion, extracts an order on the candidate terms. The robustness of\nthe approach is demonstrated on two real-world domain applications, considering\ndifferent domains (biology and human resources) and different languages\n(English and French).\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2005-12-13T13:25:57Z",
    "updated": "2005-12-13T13:25:57Z",
    "abstract_stats": {
      "total_words": 109,
      "unique_words": 82,
      "total_sentences": 5,
      "avg_words_per_sentence": 21.8,
      "avg_word_length": 5.990825688073395
    }
  },
  {
    "arxiv_id": "0512059v2",
    "title": "Competing with wild prediction rules",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  We consider the problem of on-line prediction competitive with a benchmark\nclass of continuous but highly irregular prediction rules. It is known that if\nthe benchmark class is a reproducing kernel Hilbert space, there exists a\nprediction algorithm whose average loss over the first N examples does not\nexceed the average loss of any prediction rule in the class plus a \"regret\nterm\" of O(N^(-1/2)). The elements of some natural benchmark classes, however,\nare so irregular that these classes are not Hilbert spaces. In this paper we\ndevelop Banach-space methods to construct a prediction algorithm with a regret\nterm of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to\nwhich the benchmark class fails to be a Hilbert space.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2005-12-14T20:03:30Z",
    "updated": "2006-01-25T17:36:52Z",
    "abstract_stats": {
      "total_words": 123,
      "unique_words": 76,
      "total_sentences": 4,
      "avg_words_per_sentence": 30.75,
      "avg_word_length": 4.983739837398374
    }
  },
  {
    "arxiv_id": "0601044v1",
    "title": "Genetic Programming, Validation Sets, and Parsimony Pressure",
    "authors": [
      "Christian Gagn\u00e9",
      "Marc Schoenauer",
      "Marc Parizeau",
      "Marco Tomassini"
    ],
    "abstract": "  Fitness functions based on test cases are very common in Genetic Programming\n(GP). This process can be assimilated to a learning task, with the inference of\nmodels from a limited number of samples. This paper is an investigation on two\nmethods to improve generalization in GP-based learning: 1) the selection of the\nbest-of-run individuals using a three data sets methodology, and 2) the\napplication of parsimony pressure in order to reduce the complexity of the\nsolutions. Results using GP in a binary classification setup show that while\nthe accuracy on the test sets is preserved, with less variances compared to\nbaseline results, the mean tree size obtained with the tested methods is\nsignificantly reduced.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-01-11T15:39:16Z",
    "updated": "2006-01-11T15:39:16Z",
    "abstract_stats": {
      "total_words": 114,
      "unique_words": 80,
      "total_sentences": 4,
      "avg_words_per_sentence": 28.5,
      "avg_word_length": 5.175438596491228
    }
  },
  {
    "arxiv_id": "0601087v1",
    "title": "Processing of Test Matrices with Guessing Correction",
    "authors": [
      "Kromer Victor"
    ],
    "abstract": "  It is suggested to insert into test matrix 1s for correct responses, 0s for\nresponse refusals, and negative corrective elements for incorrect responses.\nWith the classical test theory approach test scores of examinees and items are\ncalculated traditionally as sums of matrix elements, organized in rows and\ncolumns. Correlation coefficients are estimated using correction coefficients.\nIn item response theory approach examinee and item logits are estimated using\nmaximum likelihood method and probabilities of all matrix elements.\n",
    "categories": [
      "cs.LG",
      "I.2.6; K.3.2"
    ],
    "published": "2006-01-20T05:40:44Z",
    "updated": "2006-01-20T05:40:44Z",
    "abstract_stats": {
      "total_words": 76,
      "unique_words": 51,
      "total_sentences": 4,
      "avg_words_per_sentence": 19.0,
      "avg_word_length": 5.907894736842105
    }
  },
  {
    "arxiv_id": "0602062v1",
    "title": "Learning rational stochastic languages",
    "authors": [
      "Fran\u00e7ois Denis",
      "Yann Esposito",
      "Amaury Habrard"
    ],
    "abstract": "  Given a finite set of words w1,...,wn independently drawn according to a\nfixed unknown distribution law P called a stochastic language, an usual goal in\nGrammatical Inference is to infer an estimate of P in some class of\nprobabilistic models, such as Probabilistic Automata (PA). Here, we study the\nclass of rational stochastic languages, which consists in stochastic languages\nthat can be generated by Multiplicity Automata (MA) and which strictly includes\nthe class of stochastic languages generated by PA. Rational stochastic\nlanguages have minimal normal representation which may be very concise, and\nwhose parameters can be efficiently estimated from stochastic samples. We\ndesign an efficient inference algorithm DEES which aims at building a minimal\nnormal representation of the target. Despite the fact that no recursively\nenumerable class of MA computes exactly the set of rational stochastic\nlanguages over Q, we show that DEES strongly identifies tis set in the limit.\nWe study the intermediary MA output by DEES and show that they compute rational\nseries which converge absolutely to one and which can be used to provide\nstochastic languages which closely estimate the target.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-02-17T08:57:44Z",
    "updated": "2006-02-17T08:57:44Z",
    "abstract_stats": {
      "total_words": 185,
      "unique_words": 103,
      "total_sentences": 7,
      "avg_words_per_sentence": 26.428571428571427,
      "avg_word_length": 5.34054054054054
    }
  },
  {
    "arxiv_id": "0605040v1",
    "title": "General Discounting versus Average Reward",
    "authors": [
      "Marcus Hutter"
    ],
    "abstract": "  Consider an agent interacting with an environment in cycles. In every\ninteraction cycle the agent is rewarded for its performance. We compare the\naverage reward U from cycle 1 to m (average value) with the future discounted\nreward V from cycle k to infinity (discounted value). We consider essentially\narbitrary (non-geometric) discount sequences and arbitrary reward sequences\n(non-MDP environments). We show that asymptotically U for m->infinity and V for\nk->infinity are equal, provided both limits exist. Further, if the effective\nhorizon grows linearly with k or faster, then existence of the limit of U\nimplies that the limit of V exists. Conversely, if the effective horizon grows\nlinearly with k or slower, then existence of the limit of V implies that the\nlimit of U exists.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-05-09T10:39:03Z",
    "updated": "2006-05-09T10:39:03Z",
    "abstract_stats": {
      "total_words": 126,
      "unique_words": 69,
      "total_sentences": 7,
      "avg_words_per_sentence": 18.0,
      "avg_word_length": 5.126984126984127
    }
  },
  {
    "arxiv_id": "0606077v1",
    "title": "On Sequence Prediction for Arbitrary Measures",
    "authors": [
      "Daniil Ryabko",
      "Marcus Hutter"
    ],
    "abstract": "  Suppose we are given two probability measures on the set of one-way infinite\nfinite-alphabet sequences and consider the question when one of the measures\npredicts the other, that is, when conditional probabilities converge (in a\ncertain sense) when one of the measures is chosen to generate the sequence.\nThis question may be considered a refinement of the problem of sequence\nprediction in its most general formulation: for a given class of probability\nmeasures, does there exist a measure which predicts all of the measures in the\nclass? To address this problem, we find some conditions on local absolute\ncontinuity which are sufficient for prediction and which generalize several\ndifferent notions which are known to be sufficient for prediction. We also\nformulate some open questions to outline a direction for finding the conditions\non classes of measures for which prediction is possible.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-06-16T16:33:23Z",
    "updated": "2006-06-16T16:33:23Z",
    "abstract_stats": {
      "total_words": 141,
      "unique_words": 77,
      "total_sentences": 4,
      "avg_words_per_sentence": 35.25,
      "avg_word_length": 5.290780141843972
    }
  },
  {
    "arxiv_id": "0606093v1",
    "title": "Predictions as statements and decisions",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  Prediction is a complex notion, and different predictors (such as people,\ncomputer programs, and probabilistic theories) can pursue very different goals.\nIn this paper I will review some popular kinds of prediction and argue that the\ntheory of competitive on-line learning can benefit from the kinds of prediction\nthat are now foreign to it.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-06-22T04:31:51Z",
    "updated": "2006-06-22T04:31:51Z",
    "abstract_stats": {
      "total_words": 54,
      "unique_words": 43,
      "total_sentences": 2,
      "avg_words_per_sentence": 27.0,
      "avg_word_length": 5.2407407407407405
    }
  },
  {
    "arxiv_id": "0607047v1",
    "title": "PAC Classification based on PAC Estimates of Label Class Distributions",
    "authors": [
      "Nick Palmer",
      "Paul W. Goldberg"
    ],
    "abstract": "  A standard approach in pattern classification is to estimate the\ndistributions of the label classes, and then to apply the Bayes classifier to\nthe estimates of the distributions in order to classify unlabeled examples. As\none might expect, the better our estimates of the label class distributions,\nthe better the resulting classifier will be. In this paper we make this\nobservation precise by identifying risk bounds of a classifier in terms of the\nquality of the estimates of the label class distributions. We show how PAC\nlearnability relates to estimates of the distributions that have a PAC\nguarantee on their $L_1$ distance from the true distribution, and we bound the\nincrease in negative log likelihood risk in terms of PAC bounds on the\nKL-divergence. We give an inefficient but general-purpose smoothing method for\nconverting an estimated distribution that is good under the $L_1$ metric into a\ndistribution that is good under the KL-divergence.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-11T13:52:39Z",
    "updated": "2006-07-11T13:52:39Z",
    "abstract_stats": {
      "total_words": 153,
      "unique_words": 81,
      "total_sentences": 5,
      "avg_words_per_sentence": 30.6,
      "avg_word_length": 5.189542483660131
    }
  },
  {
    "arxiv_id": "0607067v1",
    "title": "Competing with stationary prediction strategies",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  In this paper we introduce the class of stationary prediction strategies and\nconstruct a prediction algorithm that asymptotically performs as well as the\nbest continuous stationary strategy. We make mild compactness assumptions but\nno stochastic assumptions about the environment. In particular, no assumption\nof stationarity is made about the environment, and the stationarity of the\nconsidered strategies only means that they do not depend explicitly on time; we\nargue that it is natural to consider only stationary strategies even for highly\nnon-stationary environments.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-13T15:52:04Z",
    "updated": "2006-07-13T15:52:04Z",
    "abstract_stats": {
      "total_words": 83,
      "unique_words": 57,
      "total_sentences": 3,
      "avg_words_per_sentence": 27.666666666666668,
      "avg_word_length": 5.843373493975903
    }
  },
  {
    "arxiv_id": "0607085v2",
    "title": "Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical\n  Inference",
    "authors": [
      "Amaury Habrard",
      "Francois Denis",
      "Yann Esposito"
    ],
    "abstract": "  In probabilistic grammatical inference, a usual goal is to infer a good\napproximation of an unknown distribution P called a stochastic language. The\nestimate of P stands in some class of probabilistic models such as\nprobabilistic automata (PA). In this paper, we focus on probabilistic models\nbased on multiplicity automata (MA). The stochastic languages generated by MA\nare called rational stochastic languages; they strictly include stochastic\nlanguages generated by PA; they also admit a very concise canonical\nrepresentation. Despite the fact that this class is not recursively enumerable,\nit is efficiently identifiable in the limit by using the algorithm DEES,\nintroduced by the authors in a previous paper. However, the identification is\nnot proper and before the convergence of the algorithm, DEES can produce MA\nthat do not define stochastic languages. Nevertheless, it is possible to use\nthese MA to define stochastic languages. We show that they belong to a broader\nclass of rational series, that we call pseudo-stochastic rational languages.\nThe aim of this paper is twofold. First we provide a theoretical study of\npseudo-stochastic rational languages, the languages output by DEES, showing for\nexample that this class is decidable within polynomial time. Second, we have\ncarried out a lot of experiments in order to compare DEES to classical\ninference algorithms such as ALERGIA and MDI. They show that DEES outperforms\nthem in most cases.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-18T07:21:51Z",
    "updated": "2008-11-07T16:21:18Z",
    "abstract_stats": {
      "total_words": 226,
      "unique_words": 120,
      "total_sentences": 12,
      "avg_words_per_sentence": 18.833333333333332,
      "avg_word_length": 5.323008849557522
    }
  },
  {
    "arxiv_id": "0607096v1",
    "title": "Logical settings for concept learning from incomplete examples in First\n  Order Logic",
    "authors": [
      "Dominique Bouthinon",
      "Henry Soldano",
      "V\u00e9ronique Ventos"
    ],
    "abstract": "  We investigate here concept learning from incomplete examples. Our first\npurpose is to discuss to what extent logical learning settings have to be\nmodified in order to cope with data incompleteness. More precisely we are\ninterested in extending the learning from interpretations setting introduced by\nL. De Raedt that extends to relational representations the classical\npropositional (or attribute-value) concept learning from examples framework. We\nare inspired here by ideas presented by H. Hirsh in a work extending the\nVersion space inductive paradigm to incomplete data. H. Hirsh proposes to\nslightly modify the notion of solution when dealing with incomplete examples: a\nsolution has to be a hypothesis compatible with all pieces of information\nconcerning the examples. We identify two main classes of incompleteness. First,\nuncertainty deals with our state of knowledge concerning an example. Second,\ngeneralization (or abstraction) deals with what part of the description of the\nexample is sufficient for the learning purpose. These two main sources of\nincompleteness can be mixed up when only part of the useful information is\nknown. We discuss a general learning setting, referred to as \"learning from\npossibilities\" that formalizes these ideas, then we present a more specific\nlearning setting, referred to as \"assumption-based learning\" that cope with\nexamples which uncertainty can be reduced when considering contextual\ninformation outside of the proper description of the examples. Assumption-based\nlearning is illustrated on a recent work concerning the prediction of a\nconsensus secondary structure common to a set of RNA sequences.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-20T14:52:08Z",
    "updated": "2006-07-20T14:52:08Z",
    "abstract_stats": {
      "total_words": 246,
      "unique_words": 130,
      "total_sentences": 14,
      "avg_words_per_sentence": 17.571428571428573,
      "avg_word_length": 5.630081300813008
    }
  },
  {
    "arxiv_id": "0607110v1",
    "title": "A Theory of Probabilistic Boosting, Decision Trees and Matryoshki",
    "authors": [
      "Etienne Grossmann"
    ],
    "abstract": "  We present a theory of boosting probabilistic classifiers. We place ourselves\nin the situation of a user who only provides a stopping parameter and a\nprobabilistic weak learner/classifier and compare three types of boosting\nalgorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of\ntrees, which we call matryoshka. \"Nested tree,\" \"embedded tree\" and \"recursive\ntree\" are also appropriate names for this algorithm, which is one of our\ncontributions. Our other contribution is the theoretical analysis of the\nalgorithms, in which we give training error bounds. This analysis suggests that\nthe matryoshka leverages probabilistic weak classifiers more efficiently than\nsimple decision trees.\n",
    "categories": [
      "cs.LG",
      "I.5.1; I.2.6; G.3"
    ],
    "published": "2006-07-25T15:57:56Z",
    "updated": "2006-07-25T15:57:56Z",
    "abstract_stats": {
      "total_words": 104,
      "unique_words": 65,
      "total_sentences": 6,
      "avg_words_per_sentence": 17.333333333333332,
      "avg_word_length": 5.6826923076923075
    }
  },
  {
    "arxiv_id": "0607134v1",
    "title": "Leading strategies in competitive on-line prediction",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  We start from a simple asymptotic result for the problem of on-line\nregression with the quadratic loss function: the class of continuous\nlimited-memory prediction strategies admits a \"leading prediction strategy\",\nwhich not only asymptotically performs at least as well as any continuous\nlimited-memory strategy but also satisfies the property that the excess loss of\nany continuous limited-memory strategy is determined by how closely it imitates\nthe leading strategy. More specifically, for any class of prediction strategies\nconstituting a reproducing kernel Hilbert space we construct a leading\nstrategy, in the sense that the loss of any prediction strategy whose norm is\nnot too large is determined by how closely it imitates the leading strategy.\nThis result is extended to the loss functions given by Bregman divergences and\nby strictly proper scoring rules.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-27T22:11:07Z",
    "updated": "2006-07-27T22:11:07Z",
    "abstract_stats": {
      "total_words": 131,
      "unique_words": 76,
      "total_sentences": 3,
      "avg_words_per_sentence": 43.666666666666664,
      "avg_word_length": 5.572519083969466
    }
  },
  {
    "arxiv_id": "0607136v1",
    "title": "Competing with Markov prediction strategies",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  Assuming that the loss function is convex in the prediction, we construct a\nprediction strategy universal for the class of Markov prediction strategies,\nnot necessarily continuous. Allowing randomization, we remove the requirement\nof convexity.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-07-28T21:45:41Z",
    "updated": "2006-07-28T21:45:41Z",
    "abstract_stats": {
      "total_words": 34,
      "unique_words": 27,
      "total_sentences": 2,
      "avg_words_per_sentence": 17.0,
      "avg_word_length": 6.0588235294117645
    }
  },
  {
    "arxiv_id": "0608033v1",
    "title": "A Study on Learnability for Rigid Lambek Grammars",
    "authors": [
      "Roberto Bonato"
    ],
    "abstract": "  We present basic notions of Gold's \"learnability in the limit\" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-08-06T16:10:05Z",
    "updated": "2006-08-06T16:10:05Z",
    "abstract_stats": {
      "total_words": 105,
      "unique_words": 72,
      "total_sentences": 3,
      "avg_words_per_sentence": 35.0,
      "avg_word_length": 5.380952380952381
    }
  },
  {
    "arxiv_id": "0609007v1",
    "title": "A Massive Local Rules Search Approach to the Classification Problem",
    "authors": [
      "Vladislav Malyshkin",
      "Ray Bakhramov",
      "Andrey Gorodetsky"
    ],
    "abstract": "  An approach to the classification problem of machine learning, based on\nbuilding local classification rules, is developed. The local rules are\nconsidered as projections of the global classification rules to the event we\nwant to classify. A massive global optimization algorithm is used for\noptimization of quality criterion. The algorithm, which has polynomial\ncomplexity in typical case, is used to find all high--quality local rules. The\nother distinctive feature of the algorithm is the integration of attributes\nlevels selection (for ordered attributes) with rules searching and original\nconflicting rules resolution strategy. The algorithm is practical; it was\ntested on a number of data sets from UCI repository, and a comparison with the\nother predicting techniques is presented.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-09-03T21:30:03Z",
    "updated": "2006-09-03T21:30:03Z",
    "abstract_stats": {
      "total_words": 117,
      "unique_words": 74,
      "total_sentences": 6,
      "avg_words_per_sentence": 19.5,
      "avg_word_length": 5.6239316239316235
    }
  },
  {
    "arxiv_id": "0609045v1",
    "title": "Metric entropy in competitive on-line prediction",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  Competitive on-line prediction (also known as universal prediction of\nindividual sequences) is a strand of learning theory avoiding making any\nstochastic assumptions about the way the observations are generated. The\npredictor's goal is to compete with a benchmark class of prediction rules,\nwhich is often a proper Banach function space. Metric entropy provides a\nunifying framework for competitive on-line prediction: the numerous known upper\nbounds on the metric entropy of various compact sets in function spaces readily\nimply bounds on the performance of on-line prediction strategies. This paper\ndiscusses strengths and limitations of the direct approach to competitive\non-line prediction via metric entropy, including comparisons to other\napproaches.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-09-09T11:31:01Z",
    "updated": "2006-09-09T11:31:01Z",
    "abstract_stats": {
      "total_words": 108,
      "unique_words": 73,
      "total_sentences": 4,
      "avg_words_per_sentence": 27.0,
      "avg_word_length": 5.953703703703703
    }
  },
  {
    "arxiv_id": "0609093v1",
    "title": "PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation\n  Assumption",
    "authors": [
      "Jon Feldman",
      "Ryan O'Donnell",
      "Rocco A. Servedio"
    ],
    "abstract": "  We propose and analyze a new vantage point for the learning of mixtures of\nGaussians: namely, the PAC-style model of learning probability distributions\nintroduced by Kearns et al. Here the task is to construct a hypothesis mixture\nof Gaussians that is statistically indistinguishable from the actual mixture\ngenerating the data; specifically, the KL-divergence should be at most epsilon.\n  In this scenario, we give a poly(n/epsilon)-time algorithm that learns the\nclass of mixtures of any constant number of axis-aligned Gaussians in\nn-dimensional Euclidean space. Our algorithm makes no assumptions about the\nseparation between the means of the Gaussians, nor does it have any dependence\non the minimum mixing weight. This is in contrast to learning results known in\nthe ``clustering'' model, where such assumptions are unavoidable.\n  Our algorithm relies on the method of moments, and a subalgorithm developed\nin previous work by the authors (FOCS 2005) for a discrete mixture-learning\nproblem.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-09-16T14:43:27Z",
    "updated": "2006-09-16T14:43:27Z",
    "abstract_stats": {
      "total_words": 150,
      "unique_words": 99,
      "total_sentences": 6,
      "avg_words_per_sentence": 25.0,
      "avg_word_length": 5.54
    }
  },
  {
    "arxiv_id": "0611011v1",
    "title": "Hedging predictions in machine learning",
    "authors": [
      "Alexander Gammerman",
      "Vladimir Vovk"
    ],
    "abstract": "  Recent advances in machine learning make it possible to design efficient\nprediction algorithms for data sets with huge numbers of parameters. This paper\ndescribes a new technique for \"hedging\" the predictions output by many such\nalgorithms, including support vector machines, kernel ridge regression, kernel\nnearest neighbours, and by many other state-of-the-art methods. The hedged\npredictions for the labels of new objects include quantitative measures of\ntheir own accuracy and reliability. These measures are provably valid under the\nassumption of randomness, traditional in machine learning: the objects and\ntheir labels are assumed to be generated independently from the same\nprobability distribution. In particular, it becomes possible to control (up to\nstatistical fluctuations) the number of erroneous predictions by selecting a\nsuitable confidence level. Validity being achieved automatically, the remaining\ngoal of hedged prediction is efficiency: taking full account of the new\nobjects' features and other available information to produce as accurate\npredictions as possible. This can be done successfully using the powerful\nmachinery of modern machine learning.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-11-02T18:44:49Z",
    "updated": "2006-11-02T18:44:49Z",
    "abstract_stats": {
      "total_words": 166,
      "unique_words": 111,
      "total_sentences": 7,
      "avg_words_per_sentence": 23.714285714285715,
      "avg_word_length": 5.993975903614458
    }
  },
  {
    "arxiv_id": "0611145v1",
    "title": "A Unified View of TD Algorithms; Introducing Full-Gradient TD and\n  Equi-Gradient Descent TD",
    "authors": [
      "Manuel Loth",
      "Philippe Preux"
    ],
    "abstract": "  This paper addresses the issue of policy evaluation in Markov Decision\nProcesses, using linear function approximation. It provides a unified view of\nalgorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is\nasserted that they all consist in minimizing a gradient function and differ by\nthe form of this function and their means of minimizing it. Two new schemes are\nintroduced in that framework: Full-gradient TD which uses a generalization of\nthe principle introduced in iLSTD, and EGD TD, which reduces the gradient by\nsuccessive equi-gradient descents. These three algorithms form a new\nintermediate family with the interesting property of making much better use of\nthe samples than TD while keeping a gradient descent scheme, which is useful\nfor complexity issues and optimistic policy iteration.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2006-11-29T00:00:57Z",
    "updated": "2006-11-29T00:00:57Z",
    "abstract_stats": {
      "total_words": 125,
      "unique_words": 82,
      "total_sentences": 5,
      "avg_words_per_sentence": 25.0,
      "avg_word_length": 5.488
    }
  },
  {
    "arxiv_id": "0703062v1",
    "title": "Bandit Algorithms for Tree Search",
    "authors": [
      "Pierre-Arnaud Coquelin",
      "R\u00e9mi Munos"
    ],
    "abstract": "  Bandit based methods for tree search have recently gained popularity when\napplied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT\nalgorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper\nConfidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to\nthe effective smoothness of the tree. However, we show that UCT is too\n``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the\ndepth of the tree. We propose alternative bandit algorithms for tree search.\nFirst, a modification of UCT using a confidence sequence that scales\nexponentially with the horizon depth is proven to have a regret O(2^D\n\\sqrt{n}), but does not adapt to possible smoothness in the tree. We then\nanalyze Flat-UCB performed on the leaves and provide a finite regret bound with\nhigh probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth\nTrees which takes into account actual smoothness of the rewards for performing\nefficient ``cuts'' of sub-optimal branches with high confidence. Finally, we\npresent an incremental tree search version which applies when the full tree is\ntoo big (possibly infinite) to be entirely represented and show that with high\nprobability, essentially only the optimal branches is indefinitely developed.\nWe illustrate these methods on a global optimization problem of a Lipschitz\nfunction, given noisy data.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-03-13T08:53:41Z",
    "updated": "2007-03-13T08:53:41Z",
    "abstract_stats": {
      "total_words": 224,
      "unique_words": 130,
      "total_sentences": 13,
      "avg_words_per_sentence": 17.23076923076923,
      "avg_word_length": 5.125
    }
  },
  {
    "arxiv_id": "0703125v1",
    "title": "Intrinsic dimension of a dataset: what properties does one expect?",
    "authors": [
      "Vladimir Pestov"
    ],
    "abstract": "  We propose an axiomatic approach to the concept of an intrinsic dimension of\na dataset, based on a viewpoint of geometry of high-dimensional structures. Our\nfirst axiom postulates that high values of dimension be indicative of the\npresence of the curse of dimensionality (in a certain precise mathematical\nsense). The second axiom requires the dimension to depend smoothly on a\ndistance between datasets (so that the dimension of a dataset and that of an\napproximating principal manifold would be close to each other). The third axiom\nis a normalization condition: the dimension of the Euclidean $n$-sphere $\\s^n$\nis $\\Theta(n)$. We give an example of a dimension function satisfying our\naxioms, even though it is in general computationally unfeasible, and discuss a\ncomputationally cheap function satisfying most but not all of our axioms (the\n``intrinsic dimensionality'' of Ch\\'avez et al.)\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-03-25T01:19:14Z",
    "updated": "2007-03-25T01:19:14Z",
    "abstract_stats": {
      "total_words": 140,
      "unique_words": 85,
      "total_sentences": 6,
      "avg_words_per_sentence": 23.333333333333332,
      "avg_word_length": 5.335714285714285
    }
  },
  {
    "arxiv_id": "0704.1274v1",
    "title": "Parametric Learning and Monte Carlo Optimization",
    "authors": [
      "David H. Wolpert",
      "Dev G. Rajnarayan"
    ],
    "abstract": "  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-04-10T17:01:07Z",
    "updated": "2007-04-10T17:01:07Z",
    "abstract_stats": {
      "total_words": 197,
      "unique_words": 112,
      "total_sentences": 11,
      "avg_words_per_sentence": 17.90909090909091,
      "avg_word_length": 5.700507614213198
    }
  },
  {
    "arxiv_id": "0704.2668v1",
    "title": "Supervised Feature Selection via Dependence Estimation",
    "authors": [
      "Le Song",
      "Alex Smola",
      "Arthur Gretton",
      "Karsten Borgwardt",
      "Justin Bedo"
    ],
    "abstract": "  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-04-20T08:26:29Z",
    "updated": "2007-04-20T08:26:29Z",
    "abstract_stats": {
      "total_words": 76,
      "unique_words": 57,
      "total_sentences": 4,
      "avg_words_per_sentence": 19.0,
      "avg_word_length": 6.078947368421052
    }
  },
  {
    "arxiv_id": "0705.1585v1",
    "title": "HMM Speaker Identification Using Linear and Non-linear Merging\n  Techniques",
    "authors": [
      "Unathi Mahola",
      "Fulufhelo V. Nelwamondo",
      "Tshilidzi Marwala"
    ],
    "abstract": "  Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-05-11T04:54:54Z",
    "updated": "2007-05-11T04:54:54Z",
    "abstract_stats": {
      "total_words": 116,
      "unique_words": 74,
      "total_sentences": 9,
      "avg_words_per_sentence": 12.88888888888889,
      "avg_word_length": 6.181034482758621
    }
  },
  {
    "arxiv_id": "0706.3679v1",
    "title": "Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers\n  Taking Values in R^Q",
    "authors": [
      "Yann Guermeur"
    ],
    "abstract": "  Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \"VC dimensions\" exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-06-25T17:28:57Z",
    "updated": "2007-06-25T17:28:57Z",
    "abstract_stats": {
      "total_words": 81,
      "unique_words": 61,
      "total_sentences": 6,
      "avg_words_per_sentence": 13.5,
      "avg_word_length": 4.753086419753086
    }
  },
  {
    "arxiv_id": "0707.3390v2",
    "title": "Consistency of the group Lasso and multiple kernel learning",
    "authors": [
      "Francis Bach"
    ],
    "abstract": "  We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-07-23T14:35:20Z",
    "updated": "2008-01-28T10:10:31Z",
    "abstract_stats": {
      "total_words": 176,
      "unique_words": 101,
      "total_sentences": 8,
      "avg_words_per_sentence": 22.0,
      "avg_word_length": 5.409090909090909
    }
  },
  {
    "arxiv_id": "0708.1242v3",
    "title": "Cost-minimising strategies for data labelling : optimal stopping and\n  active learning",
    "authors": [
      "Christos Dimitrakakis",
      "Christian Savu-Krohn"
    ],
    "abstract": "  Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-08-09T10:21:34Z",
    "updated": "2007-11-15T16:37:51Z",
    "abstract_stats": {
      "total_words": 219,
      "unique_words": 121,
      "total_sentences": 6,
      "avg_words_per_sentence": 36.5,
      "avg_word_length": 5.0228310502283104
    }
  },
  {
    "arxiv_id": "0708.1503v1",
    "title": "Defensive forecasting for optimal prediction with expert advice",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \"second-guessing\" experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-08-10T19:19:54Z",
    "updated": "2007-08-10T19:19:54Z",
    "abstract_stats": {
      "total_words": 59,
      "unique_words": 40,
      "total_sentences": 2,
      "avg_words_per_sentence": 29.5,
      "avg_word_length": 5.694915254237288
    }
  },
  {
    "arxiv_id": "0708.2353v2",
    "title": "Continuous and randomized defensive forecasting: unified view",
    "authors": [
      "Vladimir Vovk"
    ],
    "abstract": "  Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \"continuous\", in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \"randomized\", in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-08-17T12:18:24Z",
    "updated": "2007-08-23T12:44:34Z",
    "abstract_stats": {
      "total_words": 94,
      "unique_words": 60,
      "total_sentences": 3,
      "avg_words_per_sentence": 31.333333333333332,
      "avg_word_length": 5.851063829787234
    }
  },
  {
    "arxiv_id": "0709.0509v1",
    "title": "Filtering Additive Measurement Noise with Maximum Entropy in the Mean",
    "authors": [
      "Henryk Gzyl",
      "Enrique ter Horst"
    ],
    "abstract": "  The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-09-04T19:36:22Z",
    "updated": "2007-09-04T19:36:22Z",
    "abstract_stats": {
      "total_words": 70,
      "unique_words": 49,
      "total_sentences": 3,
      "avg_words_per_sentence": 23.333333333333332,
      "avg_word_length": 4.928571428571429
    }
  },
  {
    "arxiv_id": "0710.0485v2",
    "title": "Prediction with expert advice for the Brier game",
    "authors": [
      "Vladimir Vovk",
      "Fedor Zhdanov"
    ],
    "abstract": "  We show that the Brier game of prediction is mixable and find the optimal\nlearning rate and substitution function for it. The resulting prediction\nalgorithm is applied to predict results of football and tennis matches. The\ntheoretical performance guarantee turns out to be rather tight on these data\nsets, especially in the case of the more extensive tennis data.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-10-02T10:08:41Z",
    "updated": "2008-06-27T18:45:01Z",
    "abstract_stats": {
      "total_words": 59,
      "unique_words": 45,
      "total_sentences": 3,
      "avg_words_per_sentence": 19.666666666666668,
      "avg_word_length": 5.101694915254237
    }
  },
  {
    "arxiv_id": "0710.2848v1",
    "title": "Consistency of trace norm minimization",
    "authors": [
      "Francis Bach"
    ],
    "abstract": "  Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-10-15T15:38:33Z",
    "updated": "2007-10-15T15:38:33Z",
    "abstract_stats": {
      "total_words": 77,
      "unique_words": 52,
      "total_sentences": 3,
      "avg_words_per_sentence": 25.666666666666668,
      "avg_word_length": 5.142857142857143
    }
  },
  {
    "arxiv_id": "0711.3594v1",
    "title": "Clustering with Transitive Distance and K-Means Duality",
    "authors": [
      "Chunjing Xu",
      "Jianzhuang Liu",
      "Xiaoou Tang"
    ],
    "abstract": "  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-11-22T15:05:35Z",
    "updated": "2007-11-22T15:05:35Z",
    "abstract_stats": {
      "total_words": 113,
      "unique_words": 76,
      "total_sentences": 6,
      "avg_words_per_sentence": 18.833333333333332,
      "avg_word_length": 5.513274336283186
    }
  },
  {
    "arxiv_id": "0711.4452v1",
    "title": "Covariance and PCA for Categorical Variables",
    "authors": [
      "Hirotaka Niitsuma",
      "Takashi Okada"
    ],
    "abstract": "  Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-11-28T12:05:47Z",
    "updated": "2007-11-28T12:05:47Z",
    "abstract_stats": {
      "total_words": 89,
      "unique_words": 58,
      "total_sentences": 6,
      "avg_words_per_sentence": 14.833333333333334,
      "avg_word_length": 6.134831460674158
    }
  },
  {
    "arxiv_id": "0712.0130v1",
    "title": "On the Relationship between the Posterior and Optimal Similarity",
    "authors": [
      "Thomas M. Breuel"
    ],
    "abstract": "  For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.\n",
    "categories": [
      "cs.LG",
      "I.5.3; I.5.2"
    ],
    "published": "2007-12-02T09:38:26Z",
    "updated": "2007-12-02T09:38:26Z",
    "abstract_stats": {
      "total_words": 166,
      "unique_words": 91,
      "total_sentences": 6,
      "avg_words_per_sentence": 27.666666666666668,
      "avg_word_length": 6.409638554216867
    }
  },
  {
    "arxiv_id": "0712.0653v2",
    "title": "Equations of States in Singular Statistical Estimation",
    "authors": [
      "Sumio Watanabe"
    ],
    "abstract": "  Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2007-12-05T05:39:07Z",
    "updated": "2009-05-11T05:49:09Z",
    "abstract_stats": {
      "total_words": 161,
      "unique_words": 97,
      "total_sentences": 6,
      "avg_words_per_sentence": 26.833333333333332,
      "avg_word_length": 5.869565217391305
    }
  },
  {
    "arxiv_id": "0712.2869v1",
    "title": "Density estimation in linear time",
    "authors": [
      "Satyaki Mahalanabis",
      "Daniel Stefankovic"
    ],
    "abstract": "  We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-12-18T03:30:05Z",
    "updated": "2007-12-18T03:30:05Z",
    "abstract_stats": {
      "total_words": 149,
      "unique_words": 83,
      "total_sentences": 8,
      "avg_words_per_sentence": 18.625,
      "avg_word_length": 5.691275167785235
    }
  },
  {
    "arxiv_id": "0712.3402v1",
    "title": "Graph kernels between point clouds",
    "authors": [
      "Francis Bach"
    ],
    "abstract": "  Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2007-12-20T13:06:50Z",
    "updated": "2007-12-20T13:06:50Z",
    "abstract_stats": {
      "total_words": 122,
      "unique_words": 84,
      "total_sentences": 5,
      "avg_words_per_sentence": 24.4,
      "avg_word_length": 5.532786885245901
    }
  },
  {
    "arxiv_id": "0801.1988v1",
    "title": "Online variants of the cross-entropy method",
    "authors": [
      "Istvan Szita",
      "Andras Lorincz"
    ],
    "abstract": "  The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-01-14T06:56:42Z",
    "updated": "2008-01-14T06:56:42Z",
    "abstract_stats": {
      "total_words": 30,
      "unique_words": 26,
      "total_sentences": 2,
      "avg_words_per_sentence": 15.0,
      "avg_word_length": 5.033333333333333
    }
  },
  {
    "arxiv_id": "0801.4061v1",
    "title": "The optimal assignment kernel is not positive definite",
    "authors": [
      "Jean-Philippe Vert"
    ],
    "abstract": "  We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-01-26T07:32:48Z",
    "updated": "2008-01-26T07:32:48Z",
    "abstract_stats": {
      "total_words": 34,
      "unique_words": 33,
      "total_sentences": 1,
      "avg_words_per_sentence": 34.0,
      "avg_word_length": 4.852941176470588
    }
  },
  {
    "arxiv_id": "0802.1002v1",
    "title": "New Estimation Procedures for PLS Path Modelling",
    "authors": [
      "Xavier Bry"
    ],
    "abstract": "  Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \"external\" estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \"internal\" estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-02-07T15:18:27Z",
    "updated": "2008-02-07T15:18:27Z",
    "abstract_stats": {
      "total_words": 111,
      "unique_words": 74,
      "total_sentences": 7,
      "avg_words_per_sentence": 15.857142857142858,
      "avg_word_length": 5.531531531531532
    }
  },
  {
    "arxiv_id": "0802.1430v2",
    "title": "A New Approach to Collaborative Filtering: Operator Estimation with\n  Spectral Regularization",
    "authors": [
      "Jacob Abernethy",
      "Francis Bach",
      "Theodoros Evgeniou",
      "Jean-Philippe Vert"
    ],
    "abstract": "  We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \"users\" to the \"objects\" they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-02-11T12:55:34Z",
    "updated": "2008-12-19T14:05:14Z",
    "abstract_stats": {
      "total_words": 141,
      "unique_words": 85,
      "total_sentences": 7,
      "avg_words_per_sentence": 20.142857142857142,
      "avg_word_length": 5.617021276595745
    }
  },
  {
    "arxiv_id": "0804.3817v1",
    "title": "Multiple Random Oracles Are Better Than One",
    "authors": [
      "Jan Arpe",
      "Elchanan Mossel"
    ],
    "abstract": "  We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-04-23T23:18:00Z",
    "updated": "2008-04-23T23:18:00Z",
    "abstract_stats": {
      "total_words": 130,
      "unique_words": 80,
      "total_sentences": 5,
      "avg_words_per_sentence": 26.0,
      "avg_word_length": 5.115384615384615
    }
  },
  {
    "arxiv_id": "0804.4682v1",
    "title": "Introduction to Relational Networks for Classification",
    "authors": [
      "Vukosi Marivate",
      "Tshilidzi Marwala"
    ],
    "abstract": "  The use of computational intelligence techniques for classification has been\nused in numerous applications. This paper compares the use of a Multi Layer\nPerceptron Neural Network and a new Relational Network on classifying the HIV\nstatus of women at ante-natal clinics. The paper discusses the architecture of\nthe relational network and its merits compared to a neural network and most\nother computational intelligence classifiers. Results gathered from the study\nindicate comparable classification accuracies as well as revealed relationships\nbetween data features in the classification data. Much higher classification\naccuracies are recommended for future research in the area of HIV\nclassification as well as missing data estimation.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-04-29T19:25:07Z",
    "updated": "2008-04-29T19:25:07Z",
    "abstract_stats": {
      "total_words": 106,
      "unique_words": 66,
      "total_sentences": 5,
      "avg_words_per_sentence": 21.2,
      "avg_word_length": 5.915094339622642
    }
  },
  {
    "arxiv_id": "0804.4741v1",
    "title": "The Effect of Structural Diversity of an Ensemble of Classifiers on\n  Classification Accuracy",
    "authors": [
      "Lesedi Masisi",
      "Fulufhelo V. Nelwamondo",
      "Tshilidzi Marwala"
    ],
    "abstract": "  This paper aims to showcase the measure of structural diversity of an\nensemble of 9 classifiers and then map a relationship between this structural\ndiversity and accuracy. The structural diversity was induced by having\ndifferent architectures or structures of the classifiers The Genetical\nAlgorithms (GA) were used to derive the relationship between diversity and the\nclassification accuracy by evolving the classifiers and then picking 9\nclassifiers out on an ensemble of 60 classifiers. It was found that as the\nensemble became diverse the accuracy improved. However at a certain diversity\nmeasure the accuracy began to drop. The Kohavi-Wolpert variance method is used\nto measure the diversity of the ensemble. A method of voting is used to\naggregate the results from each classifier. The lowest error was observed at a\ndiversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024\nas maximum diversity measured. The parameters that were varied were: the number\nof hidden nodes, learning rate and the activation function.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-04-30T06:07:45Z",
    "updated": "2008-04-30T06:07:45Z",
    "abstract_stats": {
      "total_words": 167,
      "unique_words": 87,
      "total_sentences": 11,
      "avg_words_per_sentence": 15.181818181818182,
      "avg_word_length": 5.18562874251497
    }
  },
  {
    "arxiv_id": "0804.4898v1",
    "title": "A Quadratic Loss Multi-Class SVM",
    "authors": [
      "Emmanuel Monfrini",
      "Yann Guermeur"
    ],
    "abstract": "  Using a support vector machine requires to set two types of hyperparameters:\nthe soft margin parameter C and the parameters of the kernel. To perform this\nmodel selection task, the method of choice is cross-validation. Its\nleave-one-out variant is known to produce an estimator of the generalization\nerror which is almost unbiased. Its major drawback rests in its time\nrequirement. To overcome this difficulty, several upper bounds on the\nleave-one-out error of the pattern recognition SVM have been derived. Among\nthose bounds, the most popular one is probably the radius-margin bound. It\napplies to the hard margin pattern recognition SVM, and by extension to the\n2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\nas a direct extension of the 2-norm SVM to the multi-class case. For this\nmachine, a generalized radius-margin bound is then established.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-04-30T19:59:56Z",
    "updated": "2008-04-30T19:59:56Z",
    "abstract_stats": {
      "total_words": 140,
      "unique_words": 88,
      "total_sentences": 9,
      "avg_words_per_sentence": 15.555555555555555,
      "avg_word_length": 5.178571428571429
    }
  },
  {
    "arxiv_id": "0805.0149v1",
    "title": "On Recovery of Sparse Signals via $\\ell_1$ Minimization",
    "authors": [
      "T. Tony Cai",
      "Guangwu Xu",
      "Jun Zhang"
    ],
    "abstract": "  This article considers constrained $\\ell_1$ minimization methods for the\nrecovery of high dimensional sparse signals in three settings: noiseless,\nbounded error and Gaussian noise. A unified and elementary treatment is given\nin these noise settings for two $\\ell_1$ minimization methods: the Dantzig\nselector and $\\ell_1$ minimization with an $\\ell_2$ constraint. The results of\nthis paper improve the existing results in the literature by weakening the\nconditions and tightening the error bounds. The improvement on the conditions\nshows that signals with larger support can be recovered accurately. This paper\nalso establishes connections between restricted isometry property and the\nmutual incoherence property. Some results of Candes, Romberg and Tao (2006) and\nDonoho, Elad, and Temlyakov (2006) are extended.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-05-01T20:25:27Z",
    "updated": "2008-05-01T20:25:27Z",
    "abstract_stats": {
      "total_words": 116,
      "unique_words": 79,
      "total_sentences": 6,
      "avg_words_per_sentence": 19.333333333333332,
      "avg_word_length": 5.931034482758621
    }
  },
  {
    "arxiv_id": "0805.2752v1",
    "title": "The Margitron: A Generalised Perceptron with Margin",
    "authors": [
      "Constantinos Panagiotakopoulos",
      "Petroula Tsampouka"
    ],
    "abstract": "  We identify the classical Perceptron algorithm with margin as a member of a\nbroader family of large margin classifiers which we collectively call the\nMargitron. The Margitron, (despite its) sharing the same update rule with the\nPerceptron, is shown in an incremental setting to converge in a finite number\nof updates to solutions possessing any desirable fraction of the maximum\nmargin. Experiments comparing the Margitron with decomposition SVMs on tasks\ninvolving linear kernels and 2-norm soft margin are also reported.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-05-18T20:07:22Z",
    "updated": "2008-05-18T20:07:22Z",
    "abstract_stats": {
      "total_words": 80,
      "unique_words": 58,
      "total_sentences": 3,
      "avg_words_per_sentence": 26.666666666666668,
      "avg_word_length": 5.475
    }
  },
  {
    "arxiv_id": "0805.2775v1",
    "title": "Sample Selection Bias Correction Theory",
    "authors": [
      "Corinna Cortes",
      "Mehryar Mohri",
      "Michael Riley",
      "Afshin Rostamizadeh"
    ],
    "abstract": "  This paper presents a theoretical analysis of sample selection bias\ncorrection. The sample bias correction technique commonly used in machine\nlearning consists of reweighting the cost of an error on each training point of\na biased sample to more closely reflect the unbiased distribution. This relies\non weights derived by various estimation techniques based on finite samples. We\nanalyze the effect of an error in that estimation on the accuracy of the\nhypothesis returned by the learning algorithm for two estimation techniques: a\ncluster-based estimation technique and kernel mean matching. We also report the\nresults of sample bias correction experiments with several data sets using\nthese techniques. Our analysis is based on the novel concept of distributional\nstability which generalizes the existing concept of point-based stability. Much\nof our work and proof techniques can be used to analyze other importance\nweighting techniques and their effect on accuracy when using a distributionally\nstable algorithm.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-05-19T02:55:08Z",
    "updated": "2008-05-19T02:55:08Z",
    "abstract_stats": {
      "total_words": 153,
      "unique_words": 92,
      "total_sentences": 7,
      "avg_words_per_sentence": 21.857142857142858,
      "avg_word_length": 5.607843137254902
    }
  },
  {
    "arxiv_id": "0805.4290v1",
    "title": "From Data Topology to a Modular Classifier",
    "authors": [
      "Abdel Ennaji",
      "Arnaud Ribert",
      "Yves Lecourtier"
    ],
    "abstract": "  This article describes an approach to designing a distributed and modular\nneural classifier. This approach introduces a new hierarchical clustering that\nenables one to determine reliable regions in the representation space by\nexploiting supervised information. A multilayer perceptron is then associated\nwith each of these detected clusters and charged with recognizing elements of\nthe associated cluster while rejecting all others. The obtained global\nclassifier is comprised of a set of cooperating neural networks and completed\nby a K-nearest neighbor classifier charged with treating elements rejected by\nall the neural networks. Experimental results for the handwritten digit\nrecognition problem and comparison with neural and statistical nonmodular\nclassifiers are given.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-05-28T09:16:44Z",
    "updated": "2008-05-28T09:16:44Z",
    "abstract_stats": {
      "total_words": 108,
      "unique_words": 74,
      "total_sentences": 5,
      "avg_words_per_sentence": 21.6,
      "avg_word_length": 6.157407407407407
    }
  },
  {
    "arxiv_id": "0806.1156v1",
    "title": "Utilisation des grammaires probabilistes dans les t\u00e2ches de\n  segmentation et d'annotation prosodique",
    "authors": [
      "Irina Nesterenko",
      "St\u00e9phane Rauzy"
    ],
    "abstract": "  Nous pr\\'esentons dans cette contribution une approche \\`a la fois symbolique\net probabiliste permettant d'extraire l'information sur la segmentation du\nsignal de parole \\`a partir d'information prosodique. Nous utilisons pour ce\nfaire des grammaires probabilistes poss\\'edant une structure hi\\'erarchique\nminimale. La phase de construction des grammaires ainsi que leur pouvoir de\npr\\'ediction sont \\'evalu\\'es qualitativement ainsi que quantitativement.\n  -----\n  Methodologically oriented, the present work sketches an approach for prosodic\ninformation retrieval and speech segmentation, based on both symbolic and\nprobabilistic information. We have recourse to probabilistic grammars, within\nwhich we implement a minimal hierarchical structure. Both the stages of\nprobabilistic grammar building and its testing in prediction are explored and\nquantitatively and qualitatively evaluated.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-06-06T13:33:31Z",
    "updated": "2008-06-06T13:33:31Z",
    "abstract_stats": {
      "total_words": 114,
      "unique_words": 91,
      "total_sentences": 6,
      "avg_words_per_sentence": 19.0,
      "avg_word_length": 6.692982456140351
    }
  },
  {
    "arxiv_id": "0806.3537v2",
    "title": "Statistical Learning of Arbitrary Computable Classifiers",
    "authors": [
      "David Soloveichik"
    ],
    "abstract": "  Statistical learning theory chiefly studies restricted hypothesis classes,\nparticularly those with finite Vapnik-Chervonenkis (VC) dimension. The\nfundamental quantity of interest is the sample complexity: the number of\nsamples required to learn to a specified level of accuracy. Here we consider\nlearning over the set of all computable labeling functions. Since the\nVC-dimension is infinite and a priori (uniform) bounds on the number of samples\nare impossible, we let the learning algorithm decide when it has seen\nsufficient samples to have learned. We first show that learning in this setting\nis indeed possible, and develop a learning algorithm. We then show, however,\nthat bounding sample complexity independently of the distribution is\nimpossible. Notably, this impossibility is entirely due to the requirement that\nthe learning algorithm be computable, and not due to the statistical nature of\nthe problem.\n",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2008-06-22T01:28:14Z",
    "updated": "2008-07-10T02:51:05Z",
    "abstract_stats": {
      "total_words": 136,
      "unique_words": 85,
      "total_sentences": 7,
      "avg_words_per_sentence": 19.428571428571427,
      "avg_word_length": 5.617647058823529
    }
  },
  {
    "arxiv_id": "0806.4210v1",
    "title": "Agnostically Learning Juntas from Random Walks",
    "authors": [
      "Jan Arpe",
      "Elchanan Mossel"
    ],
    "abstract": "  We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend\non an unknown subset of k<<n variables (so-called k-juntas) is agnostically\nlearnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},\nand log(1/delta). In other words, there is an algorithm with the claimed\nrunning time that, given epsilon, delta > 0 and access to a random walk on\n{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with\nprobability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,\nwhere opt(f) denotes the distance of a closest k-junta to f.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-06-25T23:18:44Z",
    "updated": "2008-06-25T23:18:44Z",
    "abstract_stats": {
      "total_words": 101,
      "unique_words": 69,
      "total_sentences": 2,
      "avg_words_per_sentence": 50.5,
      "avg_word_length": 4.772277227722772
    }
  },
  {
    "arxiv_id": "0806.4422v1",
    "title": "Computationally Efficient Estimators for Dimension Reductions Using\n  Stable Random Projections",
    "authors": [
      "Ping Li"
    ],
    "abstract": "  The method of stable random projections is a tool for efficiently computing\nthe $l_\\alpha$ distances using low memory, where $0<\\alpha \\leq 2$ is a tuning\nparameter. The method boils down to a statistical estimation task and various\nestimators have been proposed, based on the geometric mean, the harmonic mean,\nand the fractional power etc.\n  This study proposes the optimal quantile estimator, whose main operation is\nselecting, which is considerably less expensive than taking fractional power,\nthe main operation in previous estimators. Our experiments report that the\noptimal quantile estimator is nearly one order of magnitude more\ncomputationally efficient than previous estimators. For large-scale learning\ntasks in which storing and computing pairwise distances is a serious\nbottleneck, this estimator should be desirable.\n  In addition to its computational advantages, the optimal quantile estimator\nexhibits nice theoretical properties. It is more accurate than previous\nestimators when $\\alpha>1$. We derive its theoretical error bounds and\nestablish the explicit (i.e., no hidden constants) sample complexity bound.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-06-27T05:19:19Z",
    "updated": "2008-06-27T05:19:19Z",
    "abstract_stats": {
      "total_words": 162,
      "unique_words": 108,
      "total_sentences": 10,
      "avg_words_per_sentence": 16.2,
      "avg_word_length": 5.827160493827161
    }
  },
  {
    "arxiv_id": "0806.4423v1",
    "title": "On Approximating the Lp Distances for p>2",
    "authors": [
      "Ping Li"
    ],
    "abstract": "  Applications in machine learning and data mining require computing pairwise\nLp distances in a data matrix A. For massive high-dimensional data, computing\nall pairwise distances of A can be infeasible. In fact, even storing A or all\npairwise distances of A in the memory may be also infeasible. This paper\nproposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where\np is even) distances into a sum of 2 marginal norms and p-1 ``inner products''\nat different orders. Then we apply normal or sub-Gaussian random projections to\napproximate the resultant ``inner products,'' assuming that the marginal norms\ncan be computed exactly by a linear scan. We propose two strategies for\napplying random projections. The basic projection strategy requires only one\nprojection matrix but it is more difficult to analyze, while the alternative\nprojection strategy requires p-1 projection matrices but its theoretical\nanalysis is much easier. In terms of the accuracy, at least for p=4, the basic\nstrategy is always more accurate than the alternative strategy if the data are\nnon-negative, which is common in reality.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-06-27T05:36:09Z",
    "updated": "2008-06-27T05:36:09Z",
    "abstract_stats": {
      "total_words": 182,
      "unique_words": 111,
      "total_sentences": 9,
      "avg_words_per_sentence": 20.22222222222222,
      "avg_word_length": 5.049450549450549
    }
  },
  {
    "arxiv_id": "0807.0093v1",
    "title": "Graph Kernels",
    "authors": [
      "S. V. N. Vishwanathan",
      "Karsten M. Borgwardt",
      "Imre Risi Kondor",
      "Nicol N. Schraudolph"
    ],
    "abstract": "  We present a unified framework to study graph kernels, special cases of which\ninclude the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\nmarginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\nand geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\nalgebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\nSylvester equation, we construct an algorithm that improves the time complexity\nof kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\nconjugate gradient solvers or fixed-point iterations bring our algorithm into\nthe sub-cubic domain. Experiments on graphs from bioinformatics and other\napplication domains show that it is often more than a thousand times faster\nthan previous approaches. We then explore connections between diffusion kernels\n\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\nand use these connections to propose new graph kernels. Finally, we show that\nrational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\nto graphs reduce to the random walk graph kernel.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-07-01T09:46:14Z",
    "updated": "2008-07-01T09:46:14Z",
    "abstract_stats": {
      "total_words": 151,
      "unique_words": 100,
      "total_sentences": 6,
      "avg_words_per_sentence": 25.166666666666668,
      "avg_word_length": 6.417218543046357
    }
  },
  {
    "arxiv_id": "0807.2983v1",
    "title": "On Probability Distributions for Trees: Representations, Inference and\n  Learning",
    "authors": [
      "Fran\u00e7ois Denis",
      "Amaury Habrard",
      "R\u00e9mi Gilleron",
      "Marc Tommasi",
      "\u00c9douard Gilbert"
    ],
    "abstract": "  We study probability distributions over free algebras of trees. Probability\ndistributions can be seen as particular (formal power) tree series [Berstel et\nal 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely\nstudied class of tree series is the class of rational (or recognizable) tree\nseries which can be defined either in an algebraic way or by means of\nmultiplicity tree automata. We argue that the algebraic representation is very\nconvenient to model probability distributions over a free algebra of trees.\nFirst, as in the string case, the algebraic representation allows to design\nlearning algorithms for the whole class of probability distributions defined by\nrational tree series. Note that learning algorithms for rational tree series\ncorrespond to learning algorithms for weighted tree automata where both the\nstructure and the weights are learned. Second, the algebraic representation can\nbe easily extended to deal with unranked trees (like XML trees where a symbol\nmay have an unbounded number of children). Both properties are particularly\nrelevant for applications: nondeterministic automata are required for the\ninference problem to be relevant (recall that Hidden Markov Models are\nequivalent to nondeterministic string automata); nowadays applications for Web\nInformation Extraction, Web Services and document processing consider unranked\ntrees.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-07-18T14:41:44Z",
    "updated": "2008-07-18T14:41:44Z",
    "abstract_stats": {
      "total_words": 208,
      "unique_words": 115,
      "total_sentences": 10,
      "avg_words_per_sentence": 20.8,
      "avg_word_length": 5.576923076923077
    }
  },
  {
    "arxiv_id": "0807.4198v2",
    "title": "Positive factor networks: A graphical framework for modeling\n  non-negative sequential data",
    "authors": [
      "Brian K. Vogel"
    ],
    "abstract": "  We present a novel graphical framework for modeling non-negative sequential\ndata with hierarchical structure. Our model corresponds to a network of coupled\nnon-negative matrix factorization (NMF) modules, which we refer to as a\npositive factor network (PFN). The data model is linear, subject to\nnon-negativity constraints, so that observation data consisting of an additive\ncombination of individually representable observations is also representable by\nthe network. This is a desirable property for modeling problems in\ncomputational auditory scene analysis, since distinct sound sources in the\nenvironment are often well-modeled as combining additively in the corresponding\nmagnitude spectrogram. We propose inference and learning algorithms that\nleverage existing NMF algorithms and that are straightforward to implement. We\npresent a target tracking example and provide results for synthetic observation\ndata which serve to illustrate the interesting properties of PFNs and motivate\ntheir potential usefulness in applications such as music transcription, source\nseparation, and speech recognition. We show how a target process characterized\nby a hierarchical state transition model can be represented as a PFN. Our\nresults illustrate that a PFN which is defined in terms of a single target\nobservation can then be used to effectively track the states of multiple\nsimultaneous targets. Our results show that the quality of the inferred target\nstates degrades gradually as the observation noise is increased. We also\npresent results for an example in which meaningful hierarchical features are\nextracted from a spectrogram. Such a hierarchical representation could be\nuseful for music transcription and source separation applications. We also\npropose a network for language modeling.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-07-25T22:50:46Z",
    "updated": "2009-07-16T00:30:26Z",
    "abstract_stats": {
      "total_words": 257,
      "unique_words": 139,
      "total_sentences": 12,
      "avg_words_per_sentence": 21.416666666666668,
      "avg_word_length": 5.883268482490273
    }
  },
  {
    "arxiv_id": "0810.4611v2",
    "title": "Learning Isometric Separation Maps",
    "authors": [
      "Nikolaos Vasiloglou",
      "Alexander G. Gray",
      "David V. Anderson"
    ],
    "abstract": "  Maximum Variance Unfolding (MVU) and its variants have been very successful\nin embedding data-manifolds in lower dimensional spaces, often revealing the\ntrue intrinsic dimension. In this paper we show how to also incorporate\nsupervised class information into an MVU-like method without breaking its\nconvexity. We call this method the Isometric Separation Map and we show that\nthe resulting kernel matrix can be used as a binary/multiclass Support Vector\nMachine-like method in a semi-supervised (transductive) framework. We also show\nthat the method always finds a kernel matrix that linearly separates the\ntraining data exactly without projecting them in infinite dimensional spaces.\nIn traditional SVMs we choose a kernel and hope that the data become linearly\nseparable in the kernel space. In this paper we show how the hyperplane can be\nchosen ad-hoc and the kernel is trained so that data are always linearly\nseparable. Comparisons with Large Margin SVMs show comparable performance.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-10-25T15:09:28Z",
    "updated": "2009-04-15T18:13:59Z",
    "abstract_stats": {
      "total_words": 151,
      "unique_words": 91,
      "total_sentences": 7,
      "avg_words_per_sentence": 21.571428571428573,
      "avg_word_length": 5.503311258278146
    }
  },
  {
    "arxiv_id": "0811.0139v1",
    "title": "Entropy, Perception, and Relativity",
    "authors": [
      "Stefan Jaeger"
    ],
    "abstract": "  In this paper, I expand Shannon's definition of entropy into a new form of\nentropy that allows integration of information from different random events.\nShannon's notion of entropy is a special case of my more general definition of\nentropy. I define probability using a so-called performance function, which is\nde facto an exponential distribution. Assuming that my general notion of\nentropy reflects the true uncertainty about a probabilistic event, I understand\nthat our perceived uncertainty differs. I claim that our perception is the\nresult of two opposing forces similar to the two famous antagonists in Chinese\nphilosophy: Yin and Yang. Based on this idea, I show that our perceived\nuncertainty matches the true uncertainty in points determined by the golden\nratio. I demonstrate that the well-known sigmoid function, which we typically\nemploy in artificial neural networks as a non-linear threshold function,\ndescribes the actual performance. Furthermore, I provide a motivation for the\ntime dilation in Einstein's Special Relativity, basically claiming that\nalthough time dilation conforms with our perception, it does not correspond to\nreality. At the end of the paper, I show how to apply this theoretical\nframework to practical applications. I present recognition rates for a pattern\nrecognition problem, and also propose a network architecture that can take\nadvantage of general entropy to solve complex decision problems.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-11-02T08:02:43Z",
    "updated": "2008-11-02T08:02:43Z",
    "abstract_stats": {
      "total_words": 218,
      "unique_words": 133,
      "total_sentences": 10,
      "avg_words_per_sentence": 21.8,
      "avg_word_length": 5.486238532110092
    }
  },
  {
    "arxiv_id": "0811.1629v1",
    "title": "Stability Bound for Stationary Phi-mixing and Beta-mixing Processes",
    "authors": [
      "Mehryar Mohri",
      "Afshin Rostamizadeh"
    ],
    "abstract": "  Most generalization bounds in learning theory are based on some measure of\nthe complexity of the hypothesis class used, independently of any algorithm. In\ncontrast, the notion of algorithmic stability can be used to derive tight\ngeneralization bounds that are tailored to specific learning algorithms by\nexploiting their particular properties. However, as in much of learning theory,\nexisting stability analyses and bounds apply only in the scenario where the\nsamples are independently and identically distributed. In many machine learning\napplications, however, this assumption does not hold. The observations received\nby the learning algorithm often have some inherent temporal dependence.\n  This paper studies the scenario where the observations are drawn from a\nstationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\nthe study of non-i.i.d. processes that implies a dependence between\nobservations weakening over time. We prove novel and distinct stability-based\ngeneralization bounds for stationary phi-mixing and beta-mixing sequences.\nThese bounds strictly generalize the bounds given in the i.i.d. case and apply\nto all stable learning algorithms, thereby extending the use of\nstability-bounds to non-i.i.d. scenarios.\n  We also illustrate the application of our phi-mixing generalization bounds to\ngeneral classes of learning algorithms, including Support Vector Regression,\nKernel Ridge Regression, and Support Vector Machines, and many other kernel\nregularization-based and relative entropy-based regularization algorithms.\nThese novel bounds can thus be viewed as the first theoretical basis for the\nuse of these algorithms in non-i.i.d. scenarios.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-11-11T05:09:08Z",
    "updated": "2008-11-11T05:09:08Z",
    "abstract_stats": {
      "total_words": 242,
      "unique_words": 130,
      "total_sentences": 22,
      "avg_words_per_sentence": 11.0,
      "avg_word_length": 5.8347107438016526
    }
  },
  {
    "arxiv_id": "0811.2016v1",
    "title": "Land Cover Mapping Using Ensemble Feature Selection Methods",
    "authors": [
      "A. Gidudu",
      "B. Abe",
      "T. Marwala"
    ],
    "abstract": "  Ensemble classification is an emerging approach to land cover mapping whereby\nthe final classification output is a result of a consensus of classifiers.\nIntuitively, an ensemble system should consist of base classifiers which are\ndiverse i.e. classifiers whose decision boundaries err differently. In this\npaper ensemble feature selection is used to impose diversity in ensembles. The\nfeatures of the constituent base classifiers for each ensemble were created\nthrough an exhaustive search algorithm using different separability indices.\nFor each ensemble, the classification accuracy was derived as well as a\ndiversity measure purported to give a measure of the inensemble diversity. The\ncorrelation between ensemble classification accuracy and diversity measure was\ndetermined to establish the interplay between the two variables. From the\nfindings of this paper, diversity measures as currently formulated do not\nprovide an adequate means upon which to constitute ensembles for land cover\nmapping.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-11-13T01:23:47Z",
    "updated": "2008-11-13T01:23:47Z",
    "abstract_stats": {
      "total_words": 145,
      "unique_words": 86,
      "total_sentences": 9,
      "avg_words_per_sentence": 16.11111111111111,
      "avg_word_length": 5.827586206896552
    }
  },
  {
    "arxiv_id": "0812.1357v1",
    "title": "A Novel Clustering Algorithm Based on Quantum Random Walk",
    "authors": [
      "Qiang Li",
      "Yan He",
      "Jing-ping Jiang"
    ],
    "abstract": "  The enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum random walk (QRW) with the\nproblem of data clustering, and develop two clustering algorithms based on the\none dimensional QRW. Then, the probability distributions on the positions\ninduced by QRW in these algorithms are investigated, which also indicates the\npossibility of obtaining better results. Consequently, the experimental results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms are of fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-12-07T15:22:27Z",
    "updated": "2008-12-07T15:22:27Z",
    "abstract_stats": {
      "total_words": 107,
      "unique_words": 69,
      "total_sentences": 5,
      "avg_words_per_sentence": 21.4,
      "avg_word_length": 5.757009345794392
    }
  },
  {
    "arxiv_id": "0812.1869v1",
    "title": "Convex Sparse Matrix Factorizations",
    "authors": [
      "Francis Bach",
      "Julien Mairal",
      "Jean Ponce"
    ],
    "abstract": "  We present a convex formulation of dictionary learning for sparse signal\ndecomposition. Convexity is obtained by replacing the usual explicit upper\nbound on the dictionary size by a convex rank-reducing term similar to the\ntrace norm. In particular, our formulation introduces an explicit trade-off\nbetween size and sparsity of the decomposition of rectangular matrices. Using a\nlarge set of synthetic examples, we compare the estimation abilities of the\nconvex and non-convex approaches, showing that while the convex formulation has\na single local minimum, this may lead in some cases to performance which is\ninferior to the local minima of the non-convex formulation.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-12-10T09:00:40Z",
    "updated": "2008-12-10T09:00:40Z",
    "abstract_stats": {
      "total_words": 102,
      "unique_words": 67,
      "total_sentences": 4,
      "avg_words_per_sentence": 25.5,
      "avg_word_length": 5.5
    }
  },
  {
    "arxiv_id": "0812.3145v2",
    "title": "Binary Classification Based on Potentials",
    "authors": [
      "Erik Boczko",
      "Andrew DiLullo",
      "Todd Young"
    ],
    "abstract": "  We introduce a simple and computationally trivial method for binary\nclassification based on the evaluation of potential functions. We demonstrate\nthat despite the conceptual and computational simplicity of the method its\nperformance can match or exceed that of standard Support Vector Machine\nmethods.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-12-16T20:41:06Z",
    "updated": "2008-12-16T21:05:28Z",
    "abstract_stats": {
      "total_words": 43,
      "unique_words": 35,
      "total_sentences": 2,
      "avg_words_per_sentence": 21.5,
      "avg_word_length": 5.976744186046512
    }
  },
  {
    "arxiv_id": "0812.3465v2",
    "title": "Linearly Parameterized Bandits",
    "authors": [
      "Paat Rusmevichientong",
      "John N. Tsitsiklis"
    ],
    "abstract": "  We consider bandit problems involving a large (possibly infinite) collection\nof arms, in which the expected reward of each arm is a linear function of an\n$r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$.\nThe objective is to minimize the cumulative regret and Bayes risk. When the set\nof arms corresponds to the unit sphere, we prove that the regret and Bayes risk\nis of order $\\Theta(r \\sqrt{T})$, by establishing a lower bound for an\narbitrary policy, and showing that a matching upper bound is obtained through a\npolicy that alternates between exploration and exploitation phases. The\nphase-based policy is also shown to be effective if the set of arms satisfies a\nstrong convexity condition. For the case of a general set of arms, we describe\na near-optimal policy whose regret and Bayes risk admit upper bounds of the\nform $O(r \\sqrt{T} \\log^{3/2} T)$.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-12-18T07:59:33Z",
    "updated": "2010-02-24T15:54:49Z",
    "abstract_stats": {
      "total_words": 146,
      "unique_words": 90,
      "total_sentences": 5,
      "avg_words_per_sentence": 29.2,
      "avg_word_length": 4.993150684931507
    }
  },
  {
    "arxiv_id": "0812.4952v4",
    "title": "Importance Weighted Active Learning",
    "authors": [
      "Alina Beygelzimer",
      "Sanjoy Dasgupta",
      "John Langford"
    ],
    "abstract": "  We present a practical and statistically consistent scheme for actively\nlearning binary classifiers under general loss functions. Our algorithm uses\nimportance weighting to correct sampling bias, and by controlling the variance,\nwe are able to give rigorous label complexity bounds for the learning process.\nExperiments on passively labeled data show that this approach reduces the label\ncomplexity required to achieve good predictive performance on many learning\nproblems.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2008-12-29T18:29:08Z",
    "updated": "2009-05-20T17:40:23Z",
    "abstract_stats": {
      "total_words": 67,
      "unique_words": 55,
      "total_sentences": 3,
      "avg_words_per_sentence": 22.333333333333332,
      "avg_word_length": 6.0
    }
  },
  {
    "arxiv_id": "0902.1258v1",
    "title": "Extraction de concepts sous contraintes dans des donn\u00e9es d'expression\n  de g\u00e8nes",
    "authors": [
      "Baptiste Jeudy",
      "Fran\u00e7ois Rioult"
    ],
    "abstract": "  In this paper, we propose a technique to extract constrained formal concepts.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-07T18:01:09Z",
    "updated": "2009-02-07T18:01:09Z",
    "abstract_stats": {
      "total_words": 12,
      "unique_words": 12,
      "total_sentences": 1,
      "avg_words_per_sentence": 12.0,
      "avg_word_length": 5.333333333333333
    }
  },
  {
    "arxiv_id": "0902.1259v1",
    "title": "Database Transposition for Constrained (Closed) Pattern Mining",
    "authors": [
      "Baptiste Jeudy",
      "Fran\u00e7ois Rioult"
    ],
    "abstract": "  Recently, different works proposed a new way to mine patterns in databases\nwith pathological size. For example, experiments in genome biology usually\nprovide databases with thousands of attributes (genes) but only tens of objects\n(experiments). In this case, mining the \"transposed\" database runs through a\nsmaller search space, and the Galois connection allows to infer the closed\npatterns of the original database. We focus here on constrained pattern mining\nfor those unusual databases and give a theoretical framework for database and\nconstraint transposition. We discuss the properties of constraint transposition\nand look into classical constraints. We then address the problem of generating\nthe closed patterns of the original database satisfying the constraint,\nstarting from those mined in the \"transposed\" database. Finally, we show how to\ngenerate all the patterns satisfying the constraint from the closed ones.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-07T18:01:56Z",
    "updated": "2009-02-07T18:01:56Z",
    "abstract_stats": {
      "total_words": 136,
      "unique_words": 82,
      "total_sentences": 7,
      "avg_words_per_sentence": 19.428571428571427,
      "avg_word_length": 5.698529411764706
    }
  },
  {
    "arxiv_id": "0902.1284v2",
    "title": "Multi-Label Prediction via Compressed Sensing",
    "authors": [
      "Daniel Hsu",
      "Sham M. Kakade",
      "John Langford",
      "Tong Zhang"
    ],
    "abstract": "  We consider multi-label prediction problems with large output spaces under\nthe assumption of output sparsity -- that the target (label) vectors have small\nsupport. We develop a general theory for a variant of the popular error\ncorrecting output code scheme, using ideas from compressed sensing for\nexploiting this sparsity. The method can be regarded as a simple reduction from\nmulti-label regression problems to binary regression problems. We show that the\nnumber of subproblems need only be logarithmic in the total number of possible\nlabels, making this approach radically more efficient than others. We also\nstate and prove robustness guarantees for this method in the form of regret\ntransform bounds (in general), and also provide a more detailed analysis for\nthe linear prediction setting.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-08T02:30:06Z",
    "updated": "2009-06-02T16:23:28Z",
    "abstract_stats": {
      "total_words": 123,
      "unique_words": 84,
      "total_sentences": 5,
      "avg_words_per_sentence": 24.6,
      "avg_word_length": 5.40650406504065
    }
  },
  {
    "arxiv_id": "0902.3373v1",
    "title": "Learning rules from multisource data for cardiac monitoring",
    "authors": [
      "Marie-Odile Cordier",
      "Elisa Fromont",
      "Ren\u00e9 Quiniou"
    ],
    "abstract": "  This paper formalises the concept of learning symbolic rules from multisource\ndata in a cardiac monitoring context. Our sources, electrocardiograms and\narterial blood pressure measures, describe cardiac behaviours from different\nviewpoints. To learn interpretable rules, we use an Inductive Logic Programming\n(ILP) method. We develop an original strategy to cope with the dimensionality\nissues caused by using this ILP technique on a rich multisource language. The\nresults show that our method greatly improves the feasibility and the\nefficiency of the process while staying accurate. They also confirm the\nbenefits of using multiple sources to improve the diagnosis of cardiac\narrhythmias.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-19T13:47:53Z",
    "updated": "2009-02-19T13:47:53Z",
    "abstract_stats": {
      "total_words": 100,
      "unique_words": 74,
      "total_sentences": 6,
      "avg_words_per_sentence": 16.666666666666668,
      "avg_word_length": 5.82
    }
  },
  {
    "arxiv_id": "0902.3846v1",
    "title": "Uniqueness of Low-Rank Matrix Completion by Rigidity Theory",
    "authors": [
      "Amit Singer",
      "Mihai Cucuringu"
    ],
    "abstract": "  The problem of completing a low-rank matrix from a subset of its entries is\noften encountered in the analysis of incomplete data sets exhibiting an\nunderlying factor model with applications in collaborative filtering, computer\nvision and control. Most recent work had been focused on constructing efficient\nalgorithms for exact or approximate recovery of the missing matrix entries and\nproving lower bounds for the number of known entries that guarantee a\nsuccessful recovery with high probability. A related problem from both the\nmathematical and algorithmic point of view is the distance geometry problem of\nrealizing points in a Euclidean space from a given subset of their pairwise\ndistances. Rigidity theory answers basic questions regarding the uniqueness of\nthe realization satisfying a given partial set of distances. We observe that\nbasic ideas and tools of rigidity theory can be adapted to determine uniqueness\nof low-rank matrix completion, where inner products play the role that\ndistances play in rigidity theory. This observation leads to an efficient\nrandomized algorithm for testing both local and global unique completion.\nCrucial to our analysis is a new matrix, which we call the completion matrix,\nthat serves as the analogue of the rigidity matrix.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-23T04:05:48Z",
    "updated": "2009-02-23T04:05:48Z",
    "abstract_stats": {
      "total_words": 196,
      "unique_words": 117,
      "total_sentences": 7,
      "avg_words_per_sentence": 28.0,
      "avg_word_length": 5.428571428571429
    }
  },
  {
    "arxiv_id": "0902.4127v2",
    "title": "Prediction with expert evaluators' advice",
    "authors": [
      "Alexey Chernov",
      "Vladimir Vovk"
    ],
    "abstract": "  We introduce a new protocol for prediction with expert advice in which each\nexpert evaluates the learner's and his own performance using a loss function\nthat may change over time and may be different from the loss functions used by\nthe other experts. The learner's goal is to perform better or not much worse\nthan each expert, as evaluated by that expert, for all experts simultaneously.\nIf the loss functions used by the experts are all proper scoring rules and all\nmixable, we show that the defensive forecasting algorithm enjoys the same\nperformance guarantee as that attainable by the Aggregating Algorithm in the\nstandard setting and known to be optimal. This result is also applied to the\ncase of \"specialist\" (or \"sleeping\") experts. In this case, the defensive\nforecasting algorithm reduces to a simple modification of the Aggregating\nAlgorithm.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-24T11:47:03Z",
    "updated": "2009-03-23T16:28:41Z",
    "abstract_stats": {
      "total_words": 139,
      "unique_words": 81,
      "total_sentences": 5,
      "avg_words_per_sentence": 27.8,
      "avg_word_length": 5.079136690647482
    }
  },
  {
    "arxiv_id": "0902.4228v1",
    "title": "Multiplicative updates For Non-Negative Kernel SVM",
    "authors": [
      "Vamsi K. Potluru",
      "Sergey M. Plis",
      "Morten Morup",
      "Vince D. Calhoun",
      "Terran Lane"
    ],
    "abstract": "  We present multiplicative updates for solving hard and soft margin support\nvector machines (SVM) with non-negative kernels. They follow as a natural\nextension of the updates for non-negative matrix factorization. No additional\nparam- eter setting, such as choosing learning, rate is required. Ex- periments\ndemonstrate rapid convergence to good classifiers. We analyze the rates of\nasymptotic convergence of the up- dates and establish tight bounds. We test the\nperformance on several datasets using various non-negative kernels and report\nequivalent generalization errors to that of a standard SVM.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-02-24T20:38:32Z",
    "updated": "2009-02-24T20:38:32Z",
    "abstract_stats": {
      "total_words": 87,
      "unique_words": 68,
      "total_sentences": 6,
      "avg_words_per_sentence": 14.5,
      "avg_word_length": 5.804597701149425
    }
  },
  {
    "arxiv_id": "0903.1125v1",
    "title": "Efficient Human Computation",
    "authors": [
      "Ran Gilad-Bachrach",
      "Aharon Bar-Hillel",
      "Liat Ein-Dor"
    ],
    "abstract": "  Collecting large labeled data sets is a laborious and expensive task, whose\nscaling up requires division of the labeling workload between many teachers.\nWhen the number of classes is large, miscorrespondences between the labels\ngiven by the different teachers are likely to occur, which, in the extreme\ncase, may reach total inconsistency. In this paper we describe how globally\nconsistent labels can be obtained, despite the absence of teacher coordination,\nand discuss the possible efficiency of this process in terms of human labor. We\ndefine a notion of label efficiency, measuring the ratio between the number of\nglobally consistent labels obtained and the number of labels provided by\ndistributed teachers. We show that the efficiency depends critically on the\nratio alpha between the number of data instances seen by a single teacher, and\nthe number of classes. We suggest several algorithms for the distributed\nlabeling problem, and analyze their efficiency as a function of alpha. In\naddition, we provide an upper bound on label efficiency for the case of\ncompletely uncoordinated teachers, and show that efficiency approaches 0 as the\nratio between the number of labels each teacher provides and the number of\nclasses drops (i.e. alpha goes to 0).\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-03-05T22:39:46Z",
    "updated": "2009-03-05T22:39:46Z",
    "abstract_stats": {
      "total_words": 201,
      "unique_words": 105,
      "total_sentences": 9,
      "avg_words_per_sentence": 22.333333333333332,
      "avg_word_length": 5.159203980099503
    }
  },
  {
    "arxiv_id": "0903.2299v3",
    "title": "Differential Contrastive Divergence",
    "authors": [
      "David McAllester"
    ],
    "abstract": "  This paper has been retracted.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-03-13T13:47:03Z",
    "updated": "2013-07-08T15:17:20Z",
    "abstract_stats": {
      "total_words": 5,
      "unique_words": 5,
      "total_sentences": 1,
      "avg_words_per_sentence": 5.0,
      "avg_word_length": 5.0
    }
  },
  {
    "arxiv_id": "0903.2870v2",
    "title": "On $p$-adic Classification",
    "authors": [
      "Patrick Erik Bradley"
    ],
    "abstract": "  A $p$-adic modification of the split-LBG classification method is presented\nin which first clusterings and then cluster centers are computed which locally\nminimise an energy function. The outcome for a fixed dataset is independent of\nthe prime number $p$ with finitely many exceptions. The methods are applied to\nthe construction of $p$-adic classifiers in the context of learning.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-03-16T22:52:06Z",
    "updated": "2009-06-24T14:10:45Z",
    "abstract_stats": {
      "total_words": 58,
      "unique_words": 44,
      "total_sentences": 3,
      "avg_words_per_sentence": 19.333333333333332,
      "avg_word_length": 5.5344827586206895
    }
  },
  {
    "arxiv_id": "0904.0814v1",
    "title": "Stability Analysis and Learning Bounds for Transductive Regression\n  Algorithms",
    "authors": [
      "Corinna Cortes",
      "Mehryar Mohri",
      "Dmitry Pechyony",
      "Ashish Rastogi"
    ],
    "abstract": "  This paper uses the notion of algorithmic stability to derive novel\ngeneralization bounds for several families of transductive regression\nalgorithms, both by using convexity and closed-form solutions. Our analysis\nhelps compare the stability of these algorithms. It also shows that a number of\nwidely used transductive regression algorithms are in fact unstable. Finally,\nit reports the results of experiments with local transductive regression\ndemonstrating the benefit of our stability bounds for model selection, for one\nof the algorithms, in particular for determining the radius of the local\nneighborhood used by the algorithm.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-04-05T20:08:44Z",
    "updated": "2009-04-05T20:08:44Z",
    "abstract_stats": {
      "total_words": 92,
      "unique_words": 59,
      "total_sentences": 4,
      "avg_words_per_sentence": 23.0,
      "avg_word_length": 5.793478260869565
    }
  },
  {
    "arxiv_id": "0904.2160v1",
    "title": "Inferring Dynamic Bayesian Networks using Frequent Episode Mining",
    "authors": [
      "Debprakash Patnaik",
      "Srivatsan Laxman",
      "Naren Ramakrishnan"
    ],
    "abstract": "  Motivation: Several different threads of research have been proposed for\nmodeling and mining temporal data. On the one hand, approaches such as dynamic\nBayesian networks (DBNs) provide a formal probabilistic basis to model\nrelationships between time-indexed random variables but these models are\nintractable to learn in the general case. On the other, algorithms such as\nfrequent episode mining are scalable to large datasets but do not exhibit the\nrigorous probabilistic interpretations that are the mainstay of the graphical\nmodels literature.\n  Results: We present a unification of these two seemingly diverse threads of\nresearch, by demonstrating how dynamic (discrete) Bayesian networks can be\ninferred from the results of frequent episode mining. This helps bridge the\nmodeling emphasis of the former with the counting emphasis of the latter.\nFirst, we show how, under reasonable assumptions on data characteristics and on\ninfluences of random variables, the optimal DBN structure can be computed using\na greedy, local, algorithm. Next, we connect the optimality of the DBN\nstructure with the notion of fixed-delay episodes and their counts of distinct\noccurrences. Finally, to demonstrate the practical feasibility of our approach,\nwe focus on a specific (but broadly applicable) class of networks, called\nexcitatory networks, and show how the search for the optimal DBN structure can\nbe conducted using just information from frequent episodes. Application on\ndatasets gathered from mathematical models of spiking neurons as well as real\nneuroscience datasets are presented.\n  Availability: Algorithmic implementations, simulator codebases, and datasets\nare available from our website at http://neural-code.cs.vt.edu/dbn\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-04-14T17:32:00Z",
    "updated": "2009-04-14T17:32:00Z",
    "abstract_stats": {
      "total_words": 252,
      "unique_words": 151,
      "total_sentences": 13,
      "avg_words_per_sentence": 19.384615384615383,
      "avg_word_length": 5.746031746031746
    }
  },
  {
    "arxiv_id": "0904.3664v1",
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "authors": [
      "Amnon Shashua"
    ],
    "abstract": "  Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-04-23T11:40:57Z",
    "updated": "2009-04-23T11:40:57Z",
    "abstract_stats": {
      "total_words": 30,
      "unique_words": 28,
      "total_sentences": 1,
      "avg_words_per_sentence": 30.0,
      "avg_word_length": 6.4
    }
  },
  {
    "arxiv_id": "0904.4527v1",
    "title": "Limits of Learning about a Categorical Latent Variable under Prior\n  Near-Ignorance",
    "authors": [
      "Alberto Piatti",
      "Marco Zaffalon",
      "Fabio Trojani",
      "Marcus Hutter"
    ],
    "abstract": "  In this paper, we consider the coherent theory of (epistemic) uncertainty of\nWalley, in which beliefs are represented through sets of probability\ndistributions, and we focus on the problem of modeling prior ignorance about a\ncategorical random variable. In this setting, it is a known result that a state\nof prior ignorance is not compatible with learning. To overcome this problem,\nanother state of beliefs, called \\emph{near-ignorance}, has been proposed.\nNear-ignorance resembles ignorance very closely, by satisfying some principles\nthat can arguably be regarded as necessary in a state of ignorance, and allows\nlearning to take place. What this paper does, is to provide new and substantial\nevidence that also near-ignorance cannot be really regarded as a way out of the\nproblem of starting statistical inference in conditions of very weak beliefs.\nThe key to this result is focusing on a setting characterized by a variable of\ninterest that is \\emph{latent}. We argue that such a setting is by far the most\ncommon case in practice, and we provide, for the case of categorical latent\nvariables (and general \\emph{manifest} variables) a condition that, if\nsatisfied, prevents learning to take place under prior near-ignorance. This\ncondition is shown to be easily satisfied even in the most common statistical\nproblems. We regard these results as a strong form of evidence against the\npossibility to adopt a condition of prior near-ignorance in real statistical\nproblems.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-04-29T03:16:20Z",
    "updated": "2009-04-29T03:16:20Z",
    "abstract_stats": {
      "total_words": 232,
      "unique_words": 121,
      "total_sentences": 9,
      "avg_words_per_sentence": 25.77777777777778,
      "avg_word_length": 5.262931034482759
    }
  },
  {
    "arxiv_id": "0904.4608v2",
    "title": "Temporal data mining for root-cause analysis of machine faults in\n  automotive assembly lines",
    "authors": [
      "Srivatsan Laxman",
      "Basel Shadid",
      "P. S. Sastry",
      "K. P. Unnikrishnan"
    ],
    "abstract": "  Engine assembly is a complex and heavily automated distributed-control\nprocess, with large amounts of faults data logged everyday. We describe an\napplication of temporal data mining for analyzing fault logs in an engine\nassembly plant. Frequent episode discovery framework is a model-free method\nthat can be used to deduce (temporal) correlations among events from the logs\nin an efficient manner. In addition to being theoretically elegant and\ncomputationally efficient, frequent episodes are also easy to interpret in the\nform actionable recommendations. Incorporation of domain-specific information\nis critical to successful application of the method for analyzing fault logs in\nthe manufacturing domain. We show how domain-specific knowledge can be\nincorporated using heuristic rules that act as pre-filters and post-filters to\nfrequent episode discovery. The system described here is currently being used\nin one of the engine assembly plants of General Motors and is planned for\nadaptation in other plants. To the best of our knowledge, this paper presents\nthe first real, large-scale application of temporal data mining in the\nmanufacturing domain. We believe that the ideas presented in this paper can\nhelp practitioners engineer tools for analysis in other similar or related\napplication domains as well.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-04-29T13:17:31Z",
    "updated": "2009-04-30T17:02:30Z",
    "abstract_stats": {
      "total_words": 195,
      "unique_words": 113,
      "total_sentences": 9,
      "avg_words_per_sentence": 21.666666666666668,
      "avg_word_length": 5.666666666666667
    }
  },
  {
    "arxiv_id": "0905.2347v1",
    "title": "Combining Supervised and Unsupervised Learning for GIS Classification",
    "authors": [
      "Juan-Manuel Torres-Moreno",
      "Laurent Bougrain",
      "Frd\u00e9ric Alexandre"
    ],
    "abstract": "  This paper presents a new hybrid learning algorithm for unsupervised\nclassification tasks. We combined Fuzzy c-means learning algorithm and a\nsupervised version of Minimerror to develop a hybrid incremental strategy\nallowing unsupervised classifications. We applied this new approach to a\nreal-world database in order to know if the information contained in unlabeled\nfeatures of a Geographic Information System (GIS), allows to well classify it.\nFinally, we compared our results to a classical supervised classification\nobtained by a multilayer perceptron.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-05-14T14:59:15Z",
    "updated": "2009-05-14T14:59:15Z",
    "abstract_stats": {
      "total_words": 79,
      "unique_words": 56,
      "total_sentences": 4,
      "avg_words_per_sentence": 19.75,
      "avg_word_length": 5.987341772151899
    }
  },
  {
    "arxiv_id": "0905.2997v1",
    "title": "Average-Case Active Learning with Costs",
    "authors": [
      "Andrew Guillory",
      "Jeff Bilmes"
    ],
    "abstract": "  We analyze the expected cost of a greedy active learning algorithm. Our\nanalysis extends previous work to a more general setting in which different\nqueries have different costs. Moreover, queries may have more than two possible\nresponses and the distribution over hypotheses may be non uniform. Specific\napplications include active learning with label costs, active learning for\nmulticlass and partial label queries, and batch mode active learning. We also\ndiscuss an approximate version of interest when there are very many queries.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-05-18T23:21:35Z",
    "updated": "2009-05-18T23:21:35Z",
    "abstract_stats": {
      "total_words": 81,
      "unique_words": 60,
      "total_sentences": 5,
      "avg_words_per_sentence": 16.2,
      "avg_word_length": 5.493827160493828
    }
  },
  {
    "arxiv_id": "0905.4022v1",
    "title": "Transfer Learning Using Feature Selection",
    "authors": [
      "Paramveer S. Dhillon",
      "Dean Foster",
      "Lyle Ungar"
    ],
    "abstract": "  We present three related ways of using Transfer Learning to improve feature\nselection. The three methods address different problems, and hence share\ndifferent kinds of information between tasks or feature classes, but all three\nare based on the information theoretic Minimum Description Length (MDL)\nprinciple and share the same underlying Bayesian interpretation. The first\nmethod, MIC, applies when predictive models are to be built simultaneously for\nmultiple tasks (``simultaneous transfer'') that share the same set of features.\nMIC allows each feature to be added to none, some, or all of the task models\nand is most beneficial for selecting a small set of predictive features from a\nlarge pool of features, as is common in genomic and biological datasets. Our\nsecond method, TPC (Three Part Coding), uses a similar methodology for the case\nwhen the features can be divided into feature classes. Our third method,\nTransfer-TPC, addresses the ``sequential transfer'' problem in which the task\nto which we want to transfer knowledge may not be known in advance and may have\ndifferent amounts of data than the other tasks. Transfer-TPC is most beneficial\nwhen we want to transfer knowledge between tasks which have unequal amounts of\nlabeled data, for example the data for disambiguating the senses of different\nverbs. We demonstrate the effectiveness of these approaches with experimental\nresults on real world data pertaining to genomics and to Word Sense\nDisambiguation (WSD).\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-05-25T14:29:59Z",
    "updated": "2009-05-25T14:29:59Z",
    "abstract_stats": {
      "total_words": 232,
      "unique_words": 132,
      "total_sentences": 8,
      "avg_words_per_sentence": 29.0,
      "avg_word_length": 5.306034482758621
    }
  },
  {
    "arxiv_id": "0906.0211v2",
    "title": "Equations of States in Statistical Learning for a Nonparametrizable and\n  Regular Case",
    "authors": [
      "Sumio Watanabe"
    ],
    "abstract": "  Many learning machines that have hierarchical structure or hidden variables\nare now being used in information science, artificial intelligence, and\nbioinformatics. However, several learning machines used in such fields are not\nregular but singular statistical models, hence their generalization performance\nis still left unknown. To overcome these problems, in the previous papers, we\nproved new equations in statistical learning, by which we can estimate the\nBayes generalization loss from the Bayes training loss and the functional\nvariance, on the condition that the true distribution is a singularity\ncontained in a learning machine. In this paper, we prove that the same\nequations hold even if a true distribution is not contained in a parametric\nmodel. Also we prove that, the proposed equations in a regular case are\nasymptotically equivalent to the Takeuchi information criterion. Therefore, the\nproposed equations are always applicable without any condition on the unknown\ntrue distribution.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-06-01T04:47:15Z",
    "updated": "2009-06-03T03:25:56Z",
    "abstract_stats": {
      "total_words": 148,
      "unique_words": 89,
      "total_sentences": 6,
      "avg_words_per_sentence": 24.666666666666668,
      "avg_word_length": 5.648648648648648
    }
  },
  {
    "arxiv_id": "0906.0470v1",
    "title": "An optimal linear separator for the Sonar Signals Classification task",
    "authors": [
      "Juan-Manuel Torres-Moreno",
      "Mirta B. Gordon"
    ],
    "abstract": "  The problem of classifying sonar signals from rocks and mines first studied\nby Gorman and Sejnowski has become a benchmark against which many learning\nalgorithms have been tested. We show that both the training set and the test\nset of this benchmark are linearly separable, although with different\nhyperplanes. Moreover, the complete set of learning and test patterns together,\nis also linearly separable. We give the weights that separate these sets, which\nmay be used to compare results found by other algorithms.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-06-02T11:52:36Z",
    "updated": "2009-06-02T11:52:36Z",
    "abstract_stats": {
      "total_words": 82,
      "unique_words": 61,
      "total_sentences": 4,
      "avg_words_per_sentence": 20.5,
      "avg_word_length": 5.195121951219512
    }
  },
  {
    "arxiv_id": "0906.2635v1",
    "title": "Bayesian History Reconstruction of Complex Human Gene Clusters on a\n  Phylogeny",
    "authors": [
      "Tom\u00e1\u0161 Vina\u0159",
      "Bro\u0148a Brejov\u00e1",
      "Giltae Song",
      "Adam Siepel"
    ],
    "abstract": "  Clusters of genes that have evolved by repeated segmental duplication present\ndifficult challenges throughout genomic analysis, from sequence assembly to\nfunctional analysis. Improved understanding of these clusters is of utmost\nimportance, since they have been shown to be the source of evolutionary\ninnovation, and have been linked to multiple diseases, including HIV and a\nvariety of cancers. Previously, Zhang et al. (2008) developed an algorithm for\nreconstructing parsimonious evolutionary histories of such gene clusters, using\nonly human genomic sequence data. In this paper, we propose a probabilistic\nmodel for the evolution of gene clusters on a phylogeny, and an MCMC algorithm\nfor reconstruction of duplication histories from genomic sequences in multiple\nspecies. Several projects are underway to obtain high quality BAC-based\nassemblies of duplicated clusters in multiple species, and we anticipate that\nour method will be useful in analyzing these valuable new data sets.\n",
    "categories": [
      "cs.LG",
      "G.3; J.3"
    ],
    "published": "2009-06-15T08:43:51Z",
    "updated": "2009-06-15T08:43:51Z",
    "abstract_stats": {
      "total_words": 144,
      "unique_words": 96,
      "total_sentences": 6,
      "avg_words_per_sentence": 24.0,
      "avg_word_length": 5.756944444444445
    }
  },
  {
    "arxiv_id": "0906.4032v1",
    "title": "Bayesian two-sample tests",
    "authors": [
      "Karsten M. Borgwardt",
      "Zoubin Ghahramani"
    ],
    "abstract": "  In this paper, we present two classes of Bayesian approaches to the\ntwo-sample problem. Our first class of methods extends the Bayesian t-test to\ninclude all parametric models in the exponential family and their conjugate\npriors. Our second class of methods uses Dirichlet process mixtures (DPM) of\nsuch conjugate-exponential distributions as flexible nonparametric priors over\nthe unknown distributions.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-06-22T15:25:23Z",
    "updated": "2009-06-22T15:25:23Z",
    "abstract_stats": {
      "total_words": 58,
      "unique_words": 44,
      "total_sentences": 3,
      "avg_words_per_sentence": 19.333333333333332,
      "avg_word_length": 5.913793103448276
    }
  },
  {
    "arxiv_id": "0906.4663v1",
    "title": "Acquiring Knowledge for Evaluation of Teachers Performance in Higher\n  Education using a Questionnaire",
    "authors": [
      "Hafeez Ullah Amin",
      "Abdur Rashid Khan"
    ],
    "abstract": "  In this paper, we present the step by step knowledge acquisition process by\nchoosing a structured method through using a questionnaire as a knowledge\nacquisition tool. Here we want to depict the problem domain as, how to evaluate\nteachers performance in higher education through the use of expert system\ntechnology. The problem is how to acquire the specific knowledge for a selected\nproblem efficiently and effectively from human experts and encode it in the\nsuitable computer format. Acquiring knowledge from human experts in the process\nof expert systems development is one of the most common problems cited till\nyet. This questionnaire was sent to 87 domain experts within all public and\nprivate universities in Pakistani. Among them 25 domain experts sent their\nvaluable opinions. Most of the domain experts were highly qualified, well\nexperienced and highly responsible persons. The whole questionnaire was divided\ninto 15 main groups of factors, which were further divided into 99 individual\nquestions. These facts were analyzed further to give a final shape to the\nquestionnaire. This knowledge acquisition technique may be used as a learning\ntool for further research work.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-06-25T11:09:39Z",
    "updated": "2009-06-25T11:09:39Z",
    "abstract_stats": {
      "total_words": 185,
      "unique_words": 110,
      "total_sentences": 10,
      "avg_words_per_sentence": 18.5,
      "avg_word_length": 5.318918918918919
    }
  },
  {
    "arxiv_id": "0906.5151v1",
    "title": "Unsupervised Search-based Structured Prediction",
    "authors": [
      "Hal Daum\u00e9 III"
    ],
    "abstract": "  We describe an adaptation and application of a search-based structured\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\nis possible to reduce unsupervised learning to supervised learning and\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\nadditionally show a close connection between unsupervised Searn and expectation\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\nextension. The key idea that enables this is an application of the predict-self\nidea for unsupervised learning.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-06-28T17:47:22Z",
    "updated": "2009-06-28T17:47:22Z",
    "abstract_stats": {
      "total_words": 75,
      "unique_words": 47,
      "total_sentences": 5,
      "avg_words_per_sentence": 15.0,
      "avg_word_length": 6.346666666666667
    }
  },
  {
    "arxiv_id": "0909.3609v1",
    "title": "Randomized Algorithms for Large scale SVMs",
    "authors": [
      "Vinay Jethava",
      "Krishnan Suresh",
      "Chiranjib Bhattacharyya",
      "Ramesh Hariharan"
    ],
    "abstract": "  We propose a randomized algorithm for training Support vector machines(SVMs)\non large datasets. By using ideas from Random projections we show that the\ncombinatorial dimension of SVMs is $O({log} n)$ with high probability. This\nestimate of combinatorial dimension is used to derive an iterative algorithm,\ncalled RandSVM, which at each step calls an existing solver to train SVMs on a\nrandomly chosen subset of size $O({log} n)$. The algorithm has probabilistic\nguarantees and is capable of training SVMs with Kernels for both classification\nand regression problems. Experiments done on synthetic and real life data sets\ndemonstrate that the algorithm scales up existing SVM learners, without loss of\naccuracy.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-09-19T23:40:10Z",
    "updated": "2009-09-19T23:40:10Z",
    "abstract_stats": {
      "total_words": 108,
      "unique_words": 78,
      "total_sentences": 5,
      "avg_words_per_sentence": 21.6,
      "avg_word_length": 5.5092592592592595
    }
  },
  {
    "arxiv_id": "0909.4603v1",
    "title": "Scalable Inference for Latent Dirichlet Allocation",
    "authors": [
      "James Petterson",
      "Tiberio Caetano"
    ],
    "abstract": "  We investigate the problem of learning a topic model - the well-known Latent\nDirichlet Allocation - in a distributed manner, using a cluster of C processors\nand dividing the corpus to be learned equally among them. We propose a simple\napproximated method that can be tuned, trading speed for accuracy according to\nthe task at hand. Our approach is asynchronous, and therefore suitable for\nclusters of heterogenous machines.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-09-25T05:23:33Z",
    "updated": "2009-09-25T05:23:33Z",
    "abstract_stats": {
      "total_words": 68,
      "unique_words": 54,
      "total_sentences": 3,
      "avg_words_per_sentence": 22.666666666666668,
      "avg_word_length": 5.147058823529412
    }
  },
  {
    "arxiv_id": "0910.0349v1",
    "title": "Post-Processing of Discovered Association Rules Using Ontologies",
    "authors": [
      "Claudia Marinica",
      "Fabrice Guillet",
      "Henri Briand"
    ],
    "abstract": "  In Data Mining, the usefulness of association rules is strongly limited by\nthe huge amount of delivered rules. In this paper we propose a new approach to\nprune and filter discovered rules. Using Domain Ontologies, we strengthen the\nintegration of user knowledge in the post-processing task. Furthermore, an\ninteractive and iterative framework is designed to assist the user along the\nanalyzing task. On the one hand, we represent user domain knowledge using a\nDomain Ontology over database. On the other hand, a novel technique is\nsuggested to prune and to filter discovered rules. The proposed framework was\napplied successfully over the client database provided by Nantes Habitat.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-10-02T08:40:01Z",
    "updated": "2009-10-02T08:40:01Z",
    "abstract_stats": {
      "total_words": 107,
      "unique_words": 64,
      "total_sentences": 7,
      "avg_words_per_sentence": 15.285714285714286,
      "avg_word_length": 5.271028037383178
    }
  },
  {
    "arxiv_id": "0910.0668v2",
    "title": "Variable sigma Gaussian processes: An expectation propagation\n  perspective",
    "authors": [
      "Yuan Qi",
      "Ahmed H. Abdel-Gawad",
      "Thomas P. Minka"
    ],
    "abstract": "  Gaussian processes (GPs) provide a probabilistic nonparametric representation\nof functions in regression, classification, and other problems. Unfortunately,\nexact learning with GPs is intractable for large datasets. A variety of\napproximate GP methods have been proposed that essentially map the large\ndataset into a small set of basis points. The most advanced of these, the\nvariable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have\nits own length scale. However, VSGP was only derived for regression. We\ndescribe how VSGP can be applied to classification and other problems, by\nderiving it as an expectation propagation algorithm. In this view, sparse GP\napproximations correspond to a KL-projection of the true posterior onto a\ncompact exponential family of GPs. VSGP constitutes one such family, and we\nshow how to enlarge this family to get additional accuracy. In particular, we\nshow that endowing each basis point with its own full covariance matrix\nprovides a significant increase in approximation power.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-10-05T03:30:13Z",
    "updated": "2009-10-07T21:52:48Z",
    "abstract_stats": {
      "total_words": 158,
      "unique_words": 109,
      "total_sentences": 10,
      "avg_words_per_sentence": 15.8,
      "avg_word_length": 5.417721518987341
    }
  },
  {
    "arxiv_id": "0910.2540v1",
    "title": "Effectiveness and Limitations of Statistical Spam Filters",
    "authors": [
      "M. Tariq Banday",
      "Tariq R. Jan"
    ],
    "abstract": "  In this paper we discuss the techniques involved in the design of the famous\nstatistical spam filters that include Naive Bayes, Term Frequency-Inverse\nDocument Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes\nAdditive Regression Tree. We compare these techniques with each other in terms\nof accuracy, recall, precision, etc. Further, we discuss the effectiveness and\nlimitations of statistical filters in filtering out various types of spam from\nlegitimate e-mails.\n",
    "categories": [
      "cs.LG",
      "K.6.5"
    ],
    "published": "2009-10-14T07:43:03Z",
    "updated": "2009-10-14T07:43:03Z",
    "abstract_stats": {
      "total_words": 69,
      "unique_words": 51,
      "total_sentences": 3,
      "avg_words_per_sentence": 23.0,
      "avg_word_length": 5.826086956521739
    }
  },
  {
    "arxiv_id": "0910.4683v2",
    "title": "Competing with Gaussian linear experts",
    "authors": [
      "Fedor Zhdanov",
      "Vladimir Vovk"
    ],
    "abstract": "  We study the problem of online regression. We prove a theoretical bound on\nthe square loss of Ridge Regression. We do not make any assumptions about input\nvectors or outcomes. We also show that Bayesian Ridge Regression can be thought\nof as an online algorithm competing with all the Gaussian linear experts.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-10-24T22:40:40Z",
    "updated": "2010-05-10T23:01:30Z",
    "abstract_stats": {
      "total_words": 52,
      "unique_words": 41,
      "total_sentences": 4,
      "avg_words_per_sentence": 13.0,
      "avg_word_length": 4.865384615384615
    }
  },
  {
    "arxiv_id": "0910.5461v1",
    "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs",
    "authors": [
      "Manqi Zhao",
      "Venkatesh Saligrama"
    ],
    "abstract": "  We propose a novel non-parametric adaptive anomaly detection algorithm for\nhigh dimensional data based on score functions derived from nearest neighbor\ngraphs on $n$-point nominal data. Anomalies are declared whenever the score of\na test sample falls below $\\alpha$, which is supposed to be the desired false\nalarm level. The resulting anomaly detector is shown to be asymptotically\noptimal in that it is uniformly most powerful for the specified false alarm\nlevel, $\\alpha$, for the case when the anomaly density is a mixture of the\nnominal and a known density. Our algorithm is computationally efficient, being\nlinear in dimension and quadratic in data size. It does not require choosing\ncomplicated tuning parameters or function approximation classes and it can\nadapt to local structure such as local change in dimensionality. We demonstrate\nthe algorithm on both artificial and real data sets in high dimensional feature\nspaces.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-10-28T18:46:41Z",
    "updated": "2009-10-28T18:46:41Z",
    "abstract_stats": {
      "total_words": 145,
      "unique_words": 96,
      "total_sentences": 6,
      "avg_words_per_sentence": 24.166666666666668,
      "avg_word_length": 5.36551724137931
    }
  },
  {
    "arxiv_id": "0911.0225v1",
    "title": "A Mirroring Theorem and its Application to a New Method of Unsupervised\n  Hierarchical Pattern Classification",
    "authors": [
      "Dasika Ratna Deepthi",
      "K. Eswaran"
    ],
    "abstract": "  In this paper, we prove a crucial theorem called Mirroring Theorem which\naffirms that given a collection of samples with enough information in it such\nthat it can be classified into classes and subclasses then (i) There exists a\nmapping which classifies and subclassifies these samples (ii) There exists a\nhierarchical classifier which can be constructed by using Mirroring Neural\nNetworks (MNNs) in combination with a clustering algorithm that can approximate\nthis mapping. Thus, the proof of the Mirroring theorem provides a theoretical\nbasis for the existence and a practical feasibility of constructing\nhierarchical classifiers, given the maps. Our proposed Mirroring Theorem can\nalso be considered as an extension to Kolmogrovs theorem in providing a\nrealistic solution for unsupervised classification. The techniques we develop,\nare general in nature and have led to the construction of learning machines\nwhich are (i) tree like in structure, (ii) modular (iii) with each module\nrunning on a common algorithm (tandem algorithm) and (iv) selfsupervised. We\nhave actually built the architecture, developed the tandem algorithm of such a\nhierarchical classifier and demonstrated it on an example problem.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-11-02T19:53:01Z",
    "updated": "2009-11-02T19:53:01Z",
    "abstract_stats": {
      "total_words": 182,
      "unique_words": 109,
      "total_sentences": 5,
      "avg_words_per_sentence": 36.4,
      "avg_word_length": 5.576923076923077
    }
  },
  {
    "arxiv_id": "0911.2904v4",
    "title": "Sequential anomaly detection in the presence of noise and limited\n  feedback",
    "authors": [
      "Maxim Raginsky",
      "Rebecca Willett",
      "Corinne Horn",
      "Jorge Silva",
      "Roummel Marcia"
    ],
    "abstract": "  This paper describes a methodology for detecting anomalies from sequentially\nobserved and potentially noisy data. The proposed approach consists of two main\nelements: (1) {\\em filtering}, or assigning a belief or likelihood to each\nsuccessive measurement based upon our ability to predict it from previous noisy\nobservations, and (2) {\\em hedging}, or flagging potential anomalies by\ncomparing the current belief against a time-varying and data-adaptive\nthreshold. The threshold is adjusted based on the available feedback from an\nend user. Our algorithms, which combine universal prediction with recent work\non online convex programming, do not require computing posterior distributions\ngiven all current observations and involve simple primal-dual parameter\nupdates. At the heart of the proposed approach lie exponential-family models\nwhich can be used in a wide variety of contexts and applications, and which\nyield methods that achieve sublinear per-round regret against both static and\nslowly varying product distributions with marginals drawn from the same\nexponential family. Moreover, the regret against static distributions coincides\nwith the minimax value of the corresponding online strongly convex game. We\nalso prove bounds on the number of mistakes made during the hedging step\nrelative to the best offline choice of the threshold with access to all\nestimated beliefs and feedback signals. We validate the theory on synthetic\ndata drawn from a time-varying distribution over binary vectors of high\ndimensionality, as well as on the Enron email dataset.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-11-15T18:43:10Z",
    "updated": "2012-03-13T16:11:21Z",
    "abstract_stats": {
      "total_words": 231,
      "unique_words": 154,
      "total_sentences": 8,
      "avg_words_per_sentence": 28.875,
      "avg_word_length": 5.714285714285714
    }
  },
  {
    "arxiv_id": "0911.3304v1",
    "title": "Keystroke Dynamics Authentication For Collaborative Systems",
    "authors": [
      "Romain Giot",
      "Mohamad El-Abed",
      "Christophe Rosenberger"
    ],
    "abstract": "  We present in this paper a study on the ability and the benefits of using a\nkeystroke dynamics authentication method for collaborative systems.\nAuthentication is a challenging issue in order to guarantee the security of use\nof collaborative systems during the access control step. Many solutions exist\nin the state of the art such as the use of one time passwords or smart-cards.\nWe focus in this paper on biometric based solutions that do not necessitate any\nadditional sensor. Keystroke dynamics is an interesting solution as it uses\nonly the keyboard and is invisible for users. Many methods have been published\nin this field. We make a comparative study of many of them considering the\noperational constraints of use for collaborative systems.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-11-17T13:35:40Z",
    "updated": "2009-11-17T13:35:40Z",
    "abstract_stats": {
      "total_words": 122,
      "unique_words": 75,
      "total_sentences": 7,
      "avg_words_per_sentence": 17.428571428571427,
      "avg_word_length": 5.073770491803279
    }
  },
  {
    "arxiv_id": "0911.4863v2",
    "title": "Statistical exponential families: A digest with flash cards",
    "authors": [
      "Frank Nielsen",
      "Vincent Garcia"
    ],
    "abstract": "  This document describes concisely the ubiquitous class of exponential family\ndistributions met in statistics. The first part recalls definitions and\nsummarizes main properties and duality with Bregman divergences (all proofs are\nskipped). The second part lists decompositions and related formula of common\nexponential family distributions. We recall the Fisher-Rao-Riemannian\ngeometries and the dual affine connection information geometries of statistical\nmanifolds. It is intended to maintain and update this document and catalog by\nadding new distribution items.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-11-25T14:26:54Z",
    "updated": "2011-05-13T01:52:49Z",
    "abstract_stats": {
      "total_words": 76,
      "unique_words": 58,
      "total_sentences": 5,
      "avg_words_per_sentence": 15.2,
      "avg_word_length": 6.368421052631579
    }
  },
  {
    "arxiv_id": "0912.0086v1",
    "title": "Learning Mixtures of Gaussians using the k-means Algorithm",
    "authors": [
      "Kamalika Chaudhuri",
      "Sanjoy Dasgupta",
      "Andrea Vattani"
    ],
    "abstract": "  One of the most popular algorithms for clustering in Euclidean space is the\n$k$-means algorithm; $k$-means is difficult to analyze mathematically, and few\ntheoretical guarantees are known about it, particularly when the data is {\\em\nwell-clustered}. In this paper, we attempt to fill this gap in the literature\nby analyzing the behavior of $k$-means on well-clustered data. In particular,\nwe study the case when each cluster is distributed as a different Gaussian --\nor, in other words, when the input comes from a mixture of Gaussians.\n  We analyze three aspects of the $k$-means algorithm under this assumption.\nFirst, we show that when the input comes from a mixture of two spherical\nGaussians, a variant of the 2-means algorithm successfully isolates the\nsubspace containing the means of the mixture components. Second, we show an\nexact expression for the convergence of our variant of the 2-means algorithm,\nwhen the input is a very large number of samples from a mixture of spherical\nGaussians. Our analysis does not require any lower bound on the separation\nbetween the mixture components.\n  Finally, we study the sample requirement of $k$-means; for a mixture of 2\nspherical Gaussians, we show an upper bound on the number of samples required\nby a variant of 2-means to get close to the true solution. The sample\nrequirement grows with increasing dimensionality of the data, and decreasing\nseparation between the means of the Gaussians. To match our upper bound, we\nshow an information-theoretic lower bound on any algorithm that learns mixtures\nof two spherical Gaussians; our lower bound indicates that in the case when the\noverlap between the probability masses of the two distributions is small, the\nsample requirement of $k$-means is {\\em near-optimal}.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-01T19:10:46Z",
    "updated": "2009-12-01T19:10:46Z",
    "abstract_stats": {
      "total_words": 283,
      "unique_words": 124,
      "total_sentences": 10,
      "avg_words_per_sentence": 28.3,
      "avg_word_length": 5.130742049469965
    }
  },
  {
    "arxiv_id": "0912.1198v1",
    "title": "Delay-Optimal Power and Subcarrier Allocation for OFDMA Systems via\n  Stochastic Approximation",
    "authors": [
      "Vincent K. N. Lau",
      "Ying Cui"
    ],
    "abstract": "  In this paper, we consider delay-optimal power and subcarrier allocation\ndesign for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base\nstation. There are $K$ queues at the base station for the downlink traffic to\nthe $K$ mobiles with heterogeneous packet arrivals and delay requirements. We\nshall model the problem as a $K$-dimensional infinite horizon average reward\nMarkov Decision Problem (MDP) where the control actions are assumed to be a\nfunction of the instantaneous Channel State Information (CSI) as well as the\njoint Queue State Information (QSI). This problem is challenging because it\ncorresponds to a stochastic Network Utility Maximization (NUM) problem where\ngeneral solution is still unknown. We propose an {\\em online stochastic value\niteration} solution using {\\em stochastic approximation}. The proposed power\ncontrol algorithm, which is a function of both the CSI and the QSI, takes the\nform of multi-level water-filling. We prove that under two mild conditions in\nTheorem 1 (One is the stepsize condition. The other is the condition on\naccessibility of the Markov Chain, which can be easily satisfied in most of the\ncases we are interested.), the proposed solution converges to the optimal\nsolution almost surely (with probability 1) and the proposed framework offers a\npossible solution to the general stochastic NUM problem. By exploiting the\nbirth-death structure of the queue dynamics, we obtain a reduced complexity\ndecomposed solution with linear $\\mathcal{O}(KN_F)$ complexity and\n$\\mathcal{O}(K)$ memory requirement.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-07T10:35:56Z",
    "updated": "2009-12-07T10:35:56Z",
    "abstract_stats": {
      "total_words": 235,
      "unique_words": 143,
      "total_sentences": 10,
      "avg_words_per_sentence": 23.5,
      "avg_word_length": 5.561702127659575
    }
  },
  {
    "arxiv_id": "0912.1822v1",
    "title": "Association Rule Pruning based on Interestingness Measures with\n  Clustering",
    "authors": [
      "S. Kannan",
      "R. Bhaskaran"
    ],
    "abstract": "  Association rule mining plays vital part in knowledge mining. The difficult\ntask is discovering knowledge or useful rules from the large number of rules\ngenerated for reduced support. For pruning or grouping rules, several\ntechniques are used such as rule structure cover methods, informative cover\nmethods, rule clustering, etc. Another way of selecting association rules is\nbased on interestingness measures such as support, confidence, correlation, and\nso on. In this paper, we study how rule clusters of the pattern Xi - Y are\ndistributed over different interestingness measures.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-09T18:11:11Z",
    "updated": "2009-12-09T18:11:11Z",
    "abstract_stats": {
      "total_words": 88,
      "unique_words": 62,
      "total_sentences": 5,
      "avg_words_per_sentence": 17.6,
      "avg_word_length": 5.488636363636363
    }
  },
  {
    "arxiv_id": "0912.2314v1",
    "title": "Early Detection of Breast Cancer using SVM Classifier Technique",
    "authors": [
      "Y. Ireaneus Anna Rejani",
      "S. Thamarai Selvi"
    ],
    "abstract": "  This paper presents a tumor detection algorithm from mammogram. The proposed\nsystem focuses on the solution of two problems. One is how to detect tumors as\nsuspicious regions with a very weak contrast to their background and another is\nhow to extract features which categorize tumors. The tumor detection method\nfollows the scheme of (a) mammogram enhancement. (b) The segmentation of the\ntumor area. (c) The extraction of features from the segmented tumor area. (d)\nThe use of SVM classifier. The enhancement can be defined as conversion of the\nimage quality to a better and more understandable level. The mammogram\nenhancement procedure includes filtering, top hat operation, DWT. Then the\ncontrast stretching is used to increase the contrast of the image. The\nsegmentation of mammogram images has been playing an important role to improve\nthe detection and diagnosis of breast cancer. The most common segmentation\nmethod used is thresholding. The features are extracted from the segmented\nbreast area. Next stage include, which classifies the regions using the SVM\nclassifier. The method was tested on 75 mammographic images, from the mini-MIAS\ndatabase. The methodology achieved a sensitivity of 88.75%.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-11T18:50:46Z",
    "updated": "2009-12-11T18:50:46Z",
    "abstract_stats": {
      "total_words": 189,
      "unique_words": 107,
      "total_sentences": 17,
      "avg_words_per_sentence": 11.117647058823529,
      "avg_word_length": 5.285714285714286
    }
  },
  {
    "arxiv_id": "0912.3983v1",
    "title": "Performance Analysis of AIM-K-means & K-means in Quality Cluster\n  Generation",
    "authors": [
      "Samarjeet Borah",
      "Mrinal Kanti Ghose"
    ],
    "abstract": "  Among all the partition based clustering algorithms K-means is the most\npopular and well known method. It generally shows impressive results even in\nconsiderably large data sets. The computational complexity of K-means does not\nsuffer from the size of the data set. The main disadvantage faced in performing\nthis clustering is that the selection of initial means. If the user does not\nhave adequate knowledge about the data set, it may lead to erroneous results.\nThe algorithm Automatic Initialization of Means (AIM), which is an extension to\nK-means, has been proposed to overcome the problem of initial mean generation.\nIn this paper an attempt has been made to compare the performance of the\nalgorithms through implementation\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-20T05:21:45Z",
    "updated": "2009-12-20T05:21:45Z",
    "abstract_stats": {
      "total_words": 116,
      "unique_words": 75,
      "total_sentences": 7,
      "avg_words_per_sentence": 16.571428571428573,
      "avg_word_length": 5.206896551724138
    }
  },
  {
    "arxiv_id": "0912.3995v4",
    "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and\n  Experimental Design",
    "authors": [
      "Niranjan Srinivas",
      "Andreas Krause",
      "Sham M. Kakade",
      "Matthias Seeger"
    ],
    "abstract": "  Many applications require optimizing an unknown, noisy function that is\nexpensive to evaluate. We formalize this task as a multi-armed bandit problem,\nwhere the payoff function is either sampled from a Gaussian process (GP) or has\nlow RKHS norm. We resolve the important open problem of deriving regret bounds\nfor this setting, which imply novel convergence rates for GP optimization. We\nanalyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its\ncumulative regret in terms of maximal information gain, establishing a novel\nconnection between GP optimization and experimental design. Moreover, by\nbounding the latter in terms of operator spectra, we obtain explicit sublinear\nregret bounds for many commonly used covariance functions. In some important\ncases, our bounds have surprisingly weak dependence on the dimensionality. In\nour experiments on real sensor data, GP-UCB compares favorably with other\nheuristical GP optimization approaches.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2009-12-21T00:08:19Z",
    "updated": "2010-06-09T23:24:13Z",
    "abstract_stats": {
      "total_words": 140,
      "unique_words": 104,
      "total_sentences": 7,
      "avg_words_per_sentence": 20.0,
      "avg_word_length": 5.757142857142857
    }
  },
  {
    "arxiv_id": "1001.0405v1",
    "title": "Optimal Query Complexity for Reconstructing Hypergraphs",
    "authors": [
      "Nader H. Bshouty",
      "Hanna Mazzawi"
    ],
    "abstract": "  In this paper we consider the problem of reconstructing a hidden weighted\nhypergraph of constant rank using additive queries. We prove the following: Let\n$G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$\nhyperedges. For any $m$ there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log n}{\\log m}) $$\nadditive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal\nQuery Complexity Bounds for Finding Graphs. {\\em STOC}, 749--758,~2008].\n  When the weights of the hypergraph are integers that are less than\n$O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for\nunweighted hypergraphs) there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log \\frac{n^d}{m}}{\\log\nm}). $$ additive queries.\n  Using the information theoretic bound the above query complexities are tight.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-03T19:54:40Z",
    "updated": "2010-01-03T19:54:40Z",
    "abstract_stats": {
      "total_words": 147,
      "unique_words": 82,
      "total_sentences": 12,
      "avg_words_per_sentence": 12.25,
      "avg_word_length": 5.292517006802721
    }
  },
  {
    "arxiv_id": "1001.0879v1",
    "title": "Linear Probability Forecasting",
    "authors": [
      "Fedor Zhdanov",
      "Yuri Kalnishkan"
    ],
    "abstract": "  Multi-class classification is one of the most important tasks in machine\nlearning. In this paper we consider two online multi-class classification\nproblems: classification by a linear model and by a kernelized model. The\nquality of predictions is measured by the Brier loss function. We suggest two\ncomputationally efficient algorithms to work with these problems and prove\ntheoretical guarantees on their losses. We kernelize one of the algorithms and\nprove theoretical guarantees on its loss. We perform experiments and compare\nour algorithms with logistic regression.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-06T12:40:13Z",
    "updated": "2010-01-06T12:40:13Z",
    "abstract_stats": {
      "total_words": 84,
      "unique_words": 54,
      "total_sentences": 6,
      "avg_words_per_sentence": 14.0,
      "avg_word_length": 5.726190476190476
    }
  },
  {
    "arxiv_id": "1001.1079v1",
    "title": "Measuring Latent Causal Structure",
    "authors": [
      "Ricardo Silva"
    ],
    "abstract": "  Discovering latent representations of the observed world has become\nincreasingly more relevant in data analysis. Much of the effort concentrates on\nbuilding latent variables which can be used in prediction problems, such as\nclassification and regression. A related goal of learning latent structure from\ndata is that of identifying which hidden common causes generate the\nobservations, such as in applications that require predicting the effect of\npolicies. This will be the main problem tackled in our contribution: given a\ndataset of indicators assumed to be generated by unknown and unmeasured common\ncauses, we wish to discover which hidden common causes are those, and how they\ngenerate our data. This is possible under the assumption that observed\nvariables are linear functions of the latent causes with additive noise.\nPrevious results in the literature present solutions for the case where each\nobserved variable is a noisy function of a single latent variable. We show how\nto extend the existing results for some cases where observed variables measure\nmore than one latent variable.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-07T14:41:21Z",
    "updated": "2010-01-07T14:41:21Z",
    "abstract_stats": {
      "total_words": 170,
      "unique_words": 103,
      "total_sentences": 7,
      "avg_words_per_sentence": 24.285714285714285,
      "avg_word_length": 5.364705882352941
    }
  },
  {
    "arxiv_id": "1001.2957v2",
    "title": "Asymptotic Learning Curve and Renormalizable Condition in Statistical\n  Learning Theory",
    "authors": [
      "Sumio Watanabe"
    ],
    "abstract": "  Bayes statistics and statistical physics have the common mathematical\nstructure, where the log likelihood function corresponds to the random\nHamiltonian. Recently, it was discovered that the asymptotic learning curves in\nBayes estimation are subject to a universal law, even if the log likelihood\nfunction can not be approximated by any quadratic form. However, it is left\nunknown what mathematical property ensures such a universal law. In this paper,\nwe define a renormalizable condition of the statistical estimation problem, and\nshow that, under such a condition, the asymptotic learning curves are ensured\nto be subject to the universal law, even if the true distribution is\nunrealizable and singular for a statistical model. Also we study a\nnonrenormalizable case, in which the learning curves have the different\nasymptotic behaviors from the universal law.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-18T05:34:09Z",
    "updated": "2010-03-16T04:47:17Z",
    "abstract_stats": {
      "total_words": 131,
      "unique_words": 76,
      "total_sentences": 5,
      "avg_words_per_sentence": 26.2,
      "avg_word_length": 5.480916030534351
    }
  },
  {
    "arxiv_id": "1001.3478v1",
    "title": "Role of Interestingness Measures in CAR Rule Ordering for Associative\n  Classifier: An Empirical Approach",
    "authors": [
      "S. Kannan",
      "R. Bhaskaran"
    ],
    "abstract": "  Associative Classifier is a novel technique which is the integration of\nAssociation Rule Mining and Classification. The difficult task in building\nAssociative Classifier model is the selection of relevant rules from a large\nnumber of class association rules (CARs). A very popular method of ordering\nrules for selection is based on confidence, support and antecedent size (CSA).\nOther methods are based on hybrid orderings in which CSA method is combined\nwith other measures. In the present work, we study the effect of using\ndifferent interestingness measures of Association rules in CAR rule ordering\nand selection for associative classifier.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-20T07:30:02Z",
    "updated": "2010-01-20T07:30:02Z",
    "abstract_stats": {
      "total_words": 98,
      "unique_words": 58,
      "total_sentences": 5,
      "avg_words_per_sentence": 19.6,
      "avg_word_length": 5.510204081632653
    }
  },
  {
    "arxiv_id": "1001.5007v2",
    "title": "Trajectory Clustering and an Application to Airspace Monitoring",
    "authors": [
      "Maxime Gariel",
      "Ashok N. Srivastava",
      "Eric Feron"
    ],
    "abstract": "  This paper presents a framework aimed at monitoring the behavior of aircraft\nin a given airspace. Nominal trajectories are determined and learned using data\ndriven methods. Standard procedures are used by air traffic controllers (ATC)\nto guide aircraft, ensure the safety of the airspace, and to maximize the\nrunway occupancy. Even though standard procedures are used by ATC, the control\nof the aircraft remains with the pilots, leading to a large variability in the\nflight patterns observed. Two methods to identify typical operations and their\nvariability from recorded radar tracks are presented. This knowledge base is\nthen used to monitor the conformance of current operations against operations\npreviously identified as standard. A tool called AirTrajectoryMiner is\npresented, aiming at monitoring the instantaneous health of the airspace, in\nreal time. The airspace is \"healthy\" when all aircraft are flying according to\nthe nominal procedures. A measure of complexity is introduced, measuring the\nconformance of current flight to nominal flight patterns. When an aircraft does\nnot conform, the complexity increases as more attention from ATC is required to\nensure a safe separation between aircraft.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-01-27T19:24:33Z",
    "updated": "2010-01-27T21:23:03Z",
    "abstract_stats": {
      "total_words": 182,
      "unique_words": 102,
      "total_sentences": 10,
      "avg_words_per_sentence": 18.2,
      "avg_word_length": 5.538461538461538
    }
  },
  {
    "arxiv_id": "1002.0709v1",
    "title": "Aggregating Algorithm competing with Banach lattices",
    "authors": [
      "Fedor Zhdanov",
      "Alexey Chernov",
      "Yuri Kalnishkan"
    ],
    "abstract": "  The paper deals with on-line regression settings with signals belonging to a\nBanach lattice. Our algorithms work in a semi-online setting where all the\ninputs are known in advance and outcomes are unknown and given step by step. We\napply the Aggregating Algorithm to construct a prediction method whose\ncumulative loss over all the input vectors is comparable with the cumulative\nloss of any linear functional on the Banach lattice. As a by-product we get an\nalgorithm that takes signals from an arbitrary domain. Its cumulative loss is\ncomparable with the cumulative loss of any predictor function from Besov and\nTriebel-Lizorkin spaces. We describe several applications of our setting.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-02-03T11:31:24Z",
    "updated": "2010-02-03T11:31:24Z",
    "abstract_stats": {
      "total_words": 109,
      "unique_words": 69,
      "total_sentences": 6,
      "avg_words_per_sentence": 18.166666666666668,
      "avg_word_length": 5.256880733944954
    }
  },
  {
    "arxiv_id": "1002.1144v1",
    "title": "A CHAID Based Performance Prediction Model in Educational Data Mining",
    "authors": [
      "M. Ramaswami",
      "R. Bhaskaran"
    ],
    "abstract": "  The performance in higher secondary school education in India is a turning\npoint in the academic lives of all students. As this academic performance is\ninfluenced by many factors, it is essential to develop predictive data mining\nmodel for students' performance so as to identify the slow learners and study\nthe influence of the dominant factors on their academic performance. In the\npresent investigation, a survey cum experimental methodology was adopted to\ngenerate a database and it was constructed from a primary and a secondary\nsource. While the primary data was collected from the regular students, the\nsecondary data was gathered from the school and office of the Chief Educational\nOfficer (CEO). A total of 1000 datasets of the year 2006 from five different\nschools in three different districts of Tamilnadu were collected. The raw data\nwas preprocessed in terms of filling up missing values, transforming values in\none form into another and relevant attribute/ variable selection. As a result,\nwe had 772 student records, which were used for CHAID prediction model\nconstruction. A set of prediction rules were extracted from CHIAD prediction\nmodel and the efficiency of the generated CHIAD prediction model was found. The\naccuracy of the present model was compared with other model and it has been\nfound to be satisfactory.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-02-05T08:27:17Z",
    "updated": "2010-02-05T08:27:17Z",
    "abstract_stats": {
      "total_words": 214,
      "unique_words": 120,
      "total_sentences": 9,
      "avg_words_per_sentence": 23.77777777777778,
      "avg_word_length": 5.163551401869159
    }
  },
  {
    "arxiv_id": "1002.1156v1",
    "title": "Dimensionality Reduction: An Empirical Study on the Usability of IFE-CF\n  (Independent Feature Elimination- by C-Correlation and F-Correlation)\n  Measures",
    "authors": [
      "M. Babu Reddy",
      "L. S. S. Reddy"
    ],
    "abstract": "  The recent increase in dimensionality of data has thrown a great challenge to\nthe existing dimensionality reduction methods in terms of their effectiveness.\nDimensionality reduction has emerged as one of the significant preprocessing\nsteps in machine learning applications and has been effective in removing\ninappropriate data, increasing learning accuracy, and improving\ncomprehensibility. Feature redundancy exercises great influence on the\nperformance of classification process. Towards the better classification\nperformance, this paper addresses the usefulness of truncating the highly\ncorrelated and redundant attributes. Here, an effort has been made to verify\nthe utility of dimensionality reduction by applying LVQ (Learning Vector\nQuantization) method on two Benchmark datasets of 'Pima Indian Diabetic\npatients' and 'Lung cancer patients'.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-02-05T08:59:05Z",
    "updated": "2010-02-05T08:59:05Z",
    "abstract_stats": {
      "total_words": 114,
      "unique_words": 78,
      "total_sentences": 5,
      "avg_words_per_sentence": 22.8,
      "avg_word_length": 6.37719298245614
    }
  },
  {
    "arxiv_id": "1002.1782v3",
    "title": "Online Distributed Sensor Selection",
    "authors": [
      "Daniel Golovin",
      "Matthew Faulkner",
      "Andreas Krause"
    ],
    "abstract": "  A key problem in sensor networks is to decide which sensors to query when, in\norder to obtain the most useful information (e.g., for performing accurate\nprediction), subject to constraints (e.g., on power and bandwidth). In many\napplications the utility function is not known a priori, must be learned from\ndata, and can even change over time. Furthermore for large sensor networks\nsolving a centralized optimization problem to select sensors is not feasible,\nand thus we seek a fully distributed solution. In this paper, we present\nDistributed Online Greedy (DOG), an efficient, distributed algorithm for\nrepeatedly selecting sensors online, only receiving feedback about the utility\nof the selected sensors. We prove very strong theoretical no-regret guarantees\nthat apply whenever the (unknown) utility function satisfies a natural\ndiminishing returns property called submodularity. Our algorithm has extremely\nlow communication requirements, and scales well to large sensor deployments. We\nextend DOG to allow observation-dependent sensor selection. We empirically\ndemonstrate the effectiveness of our algorithm on several real-world sensing\ntasks.\n",
    "categories": [
      "cs.LG"
    ],
    "published": "2010-02-09T07:32:59Z",
    "updated": "2010-05-13T03:32:05Z",
    "abstract_stats": {
      "total_words": 168,
      "unique_words": 116,
      "total_sentences": 12,
      "avg_words_per_sentence": 14.0,
      "avg_word_length": 5.732142857142857
    }
  }
]